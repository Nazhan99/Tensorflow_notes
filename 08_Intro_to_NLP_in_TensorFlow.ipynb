{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_Intro_to_NLP_in_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mrHoSfDSe28R",
        "dQ34LaiYhMB_",
        "4j_6FIARASgZ",
        "cYp-egarD5VK",
        "ThpmP9DzEiTt",
        "rYr6LKIJcZVh"
      ],
      "authorship_tag": "ABX9TyP5Hv45KYYZJdb8CpyRycN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nazhan99/Tensorflow_notes/blob/main/08_Intro_to_NLP_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to NLP Fundamentals in TensorFlow\n",
        "\n",
        "NLP has the goal of deriving information out of natural language (could be sdequences text or speech)\n",
        "\n",
        "Another common term for NLP problems is sequence to sequence problems (seq2seq)"
      ],
      "metadata": {
        "id": "ameWPStKSyv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check for GPU\n"
      ],
      "metadata": {
        "id": "fgDfctsmaGUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "nrq0GpVZaI_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3ed75a-d385-40ed-af5a-32fe4b767fff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar  7 06:34:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get helper functions"
      ],
      "metadata": {
        "id": "cUKdCJpDaKZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "#import series of helper functions for the notebook\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves , compare_historys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDVyxfeEaX_o",
        "outputId": "2808c066-76c8-499a-9a77-21c47fb080c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-07 06:34:15--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-07 06:34:15 (48.0 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get a text dataset\n",
        "\n",
        "The data set we are going to be using is Kaggle's introduction to NLP dataset (text samples of tweets labelled as disaster or not disaster)"
      ],
      "metadata": {
        "id": "U3U3COcuar1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
        "\n",
        "#unzip data\n",
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0DSE-gIbDPQ",
        "outputId": "33ee3b02-d0f9-47f1-87e1-2be9852608d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-07 06:34:22--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.147.128, 142.250.136.128, 142.250.148.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.147.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-03-07 06:34:22 (42.2 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize a text dataset"
      ],
      "metadata": {
        "id": "jXem76dpbUBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "-1zdLIFYbt_4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jiBx3Raob9A-",
        "outputId": "f550a468-20cf-40bc-d7b0-684e365c12c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-27c7ef56-5b5f-42a1-8d9c-1f4d071d716c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27c7ef56-5b5f-42a1-8d9c-1f4d071d716c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27c7ef56-5b5f-42a1-8d9c-1f4d071d716c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27c7ef56-5b5f-42a1-8d9c-1f4d071d716c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"text\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K7lDGwXpchVB",
        "outputId": "3c9bddac-8304-428f-a3e3-e63c80417890"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Forest fire near La Ronge Sask. Canada'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle training dataframe\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
        "train_df_shuffled.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VyeJCpYeckQz",
        "outputId": "8ccf2e8f-b249-4110-d456-b3daa95a06f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-77a0d449-4156-40c1-8804-5b5ca85dbcc5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77a0d449-4156-40c1-8804-5b5ca85dbcc5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-77a0d449-4156-40c1-8804-5b5ca85dbcc5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-77a0d449-4156-40c1-8804-5b5ca85dbcc5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id      keyword               location  \\\n",
              "2644  3796  destruction                    NaN   \n",
              "2227  3185       deluge                    NaN   \n",
              "5448  7769       police                     UK   \n",
              "132    191   aftershock                    NaN   \n",
              "6845  9810       trauma  Montgomery County, MD   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dMF00FTQdA5O",
        "outputId": "cf34b43b-1a8e-441a-d2ce-1fd462b03efe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fbd023e2-4a34-4bb8-a209-8a3a97e6dd35\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbd023e2-4a34-4bb8-a209-8a3a97e6dd35')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbd023e2-4a34-4bb8-a209-8a3a97e6dd35 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbd023e2-4a34-4bb8-a209-8a3a97e6dd35');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#how many examples of each class?\n",
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLY2wHEPdNmX",
        "outputId": "05d4d299-e364-4ec4-84fa-9fd306e2e22b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#how many total samples?\n",
        "len(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcrsy9uSdX53",
        "outputId": "9afdd45d-cf45-4765-eeed-1c16ca7385db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7613"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kALtI6fQdzCN",
        "outputId": "6983d2f5-7531-4792-f902-e9630805bef0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3263"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's visualize some random training examples\n",
        "import random \n",
        "random_index= random.randint(0,len(train_df)-5) #create random indexes not higeher than total samples\n",
        "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
        "  print(f\"Text:\\n {text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMeG9QbTd0x4",
        "outputId": "69bfb7f5-0461-4c4d-9d4d-c76a60d0e37e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            " London is cool ;)\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "Text:\n",
            " 70 Years After Atomic Bombs Japan Still Struggles With War Past: The anniversary of the devastation wrought b... http://t.co/ohNdh2rI0V\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            " infected bloody ear piercings are always fun??\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            " Marlon Williams &gt; Elvis Presley &gt; Marlon Williams &gt; Steel Panther. \n",
            "\n",
            "Shuffle mode like a bloody legend.\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            " I love The body shopÛªs bags??\n",
            "\n",
            "#cutekitten #catsofinstagram #summerinsweden #katt #katterpÌ´instagram #dumle #dagensÛ_ http://t.co/p4ZFXdnbcH\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into training and validation sets"
      ],
      "metadata": {
        "id": "mrHoSfDSe28R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "xj-i8jrpfxQq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use train_test_split to split training data into training and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                            test_size=0.1, #use 10% of training data for validation\n",
        "                                            random_state=42)"
      ],
      "metadata": {
        "id": "h4Z4Ce0-gCmR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the lengths\n",
        "len(train_sentences) , len(val_sentences), len(train_labels), len(val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx9gzSZfgxPt",
        "outputId": "f7b62f4b-a7f5-4163-fe5f-4e16b05434bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 762, 6851, 762)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the first 10 samples \n",
        "train_sentences[:10], train_labels[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll-S6ueNhBiI",
        "outputId": "e47e5ccd-83e3-4b04-a389-7ccf713a3880"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting text into numbers\n",
        "\n",
        "When dealing with  a text problem, one of the first things you will have to do before you can build a model is to conver tthe text to numbers.\n",
        "\n",
        "There are a few way to do:\n",
        "\n",
        "* Tokenization - direct mapping of token( a token could be a word or a character) to number\n",
        "* Embedding - create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)"
      ],
      "metadata": {
        "id": "dQ34LaiYhMB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text vectorization (tokenization)"
      ],
      "metadata": {
        "id": "GZ73-iV144id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fTfg6Dy5B9h",
        "outputId": "cd2a42bc-3afe-4c69-b325-2bde23cd85ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "       'Imagine getting flattened by Kurt Zouma',\n",
              "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "#https://tensorflow.google.cn/api_docs/python/tf/keras/layers/TextVectorization"
      ],
      "metadata": {
        "id": "onxImDb35GH1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the default TextVectorization parameters\n",
        "text_vectorizer = TextVectorization(max_tokens=50,# how many words in the vocabulary (automatically add <OOV>)\n",
        "                                    standardize=\"lower_and_strip_punctuation\",\n",
        "                                    split=\"whitespace\",\n",
        "                                    ngrams=None, #create groups of n-words?\n",
        "                                    output_mode=\"int\" ,#how to map tokens to numbers\n",
        "                                    output_sequence_length=None, #how long do you want your sequences to be\n",
        "                                    pad_to_max_tokens=True)"
      ],
      "metadata": {
        "id": "lyUwe-Hf5alD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the average number of tokens (words) in the training tweets\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX2GGOAe7gug",
        "outputId": "cd73f07a-02f1-4129-94bf-60e229738eb1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup text vectorization variables\n",
        "max_vocab_length = 10000 #max number of words to have in our vocabualry\n",
        "max_length= 15 #max length our sequences will be ( how many words from a tweet does a model see?)\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                     output_mode=\"int\",\n",
        "                                     output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "gwBeWKAn8r3e"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the text vectorizer \n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "pke-G92S9kwe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a sample sentence and tokenize it \n",
        "sample_sentence = \"There's a flood in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d2qPuJ9-LXW",
        "outputId": "d041a8f7-5576-40b2-caa2-d5c9cdc3946e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose a random sentence from the training dataset and tokenize it\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n {random_sentence}\\\n",
        "        \\n\\nVectorized version:\")\n",
        "text_vectorizer([random_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH2fqVJA-Nxh",
        "outputId": "44fac661-5ae0-4b18-bf2c-60928315053e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            " Greg Garza not in the 18 for Atlas tonight vs Leones Negros in Copa MX play. He left the previous game w/ an injury. #USMNT        \n",
            "\n",
            "Vectorized version:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[3805,    1,   34,    4,    2, 1339,   10,    1,  383,  975,    1,\n",
              "           1,    4,    1,    1]])>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary() # get all the unique word \n",
        "top_5_words = words_in_vocab[:5] #get the most common words\n",
        "bottom_5_words = words_in_vocab[:-5] # get the least common words\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"5 most common words: {len(top_5_words)}, {top_5_words}\")\n",
        "print(f\"5 least common words: {len(bottom_5_words)}, {bottom_5_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glhURh3X_Q_c",
        "outputId": "7f589c02-025b-4af6-e1b7-02bd7ef8e1e0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in vocab: 10000\n",
            "5 most common words: 5, ['', '[UNK]', 'the', 'a', 'in']\n",
            "5 least common words: 9995, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is', 'for', 'on', 'you', 'my', 'with', 'it', 'that', 'at', 'by', 'this', 'from', 'be', 'are', 'was', 'have', 'like', 'as', 'up', 'so', 'just', 'but', 'me', 'im', 'your', 'not', 'amp', 'out', 'its', 'will', 'an', 'no', 'has', 'fire', 'after', 'all', 'when', 'we', 'if', 'now', 'via', 'new', 'more', 'get', 'or', 'about', 'what', 'he', 'people', 'news', 'been', 'over', 'one', 'how', 'dont', 'they', 'who', 'into', 'were', 'do', 'us', '2', 'can', 'video', 'emergency', 'there', 'disaster', 'than', 'police', 'would', 'his', 'still', 'her', 'some', 'body', 'storm', 'crash', 'burning', 'suicide', 'back', 'man', 'california', 'why', 'time', 'them', 'had', 'buildings', 'rt', 'first', 'cant', 'see', 'got', 'day', 'off', 'our', 'going', 'nuclear', 'know', 'world', 'bomb', 'fires', 'love', 'killed', 'go', 'attack', 'youtube', 'dead', 'two', 'families', '3', 'train', 'full', 'being', 'war', 'many', 'today', 'think', 'only', 'car', 'accident', 'life', 'hiroshima', 'their', 'say', 'may', 'down', 'watch', 'good', 'could', 'want', 'last', 'here', 'years', 'u', 'then', 'make', 'did', 'wildfire', 'way', 'help', 'best', 'too', 'even', 'because', 'home', 'death', 'collapse', 'bombing', 'mass', 'him', 'black', 'am', 'those', 'need', 'fatal', 'army', 'another', 'work', 'take', 'should', 'really', 'please', 'mh370', 'youre', 'look', 'lol', 'hot', 'pm', 'legionnaires', '4', 'right', '5', 'let', 'city', 'year', 'wreck', 'school', 'northern', 'much', 'forest', 'bomber', 'water', 'she', 'never', 'read', 'latest', 'homes', 'great', 'every', '1', 'live', 'god', 'fear', 'any', '\\x89Û', 'under', 'said', 'old', 'floods', '2015', 'getting', 'atomic', 'while', 'top', 'obama', 'feel', 'thats', 'since', 'near', 'flames', 'ever', 'come', 'where', 'these', 'military', 'japan', 'found', 'content', 'ass', 'without', 'weather', 'most', 'flooding', 'flood', 'damage', 'which', 'shit', 's', 'hope', 'everyone', 'before', 'stop', 'plan', 'malaysia', 'injured', 'hit', 'evacuation', 'during', 'debris', 'cross', 'coming', 'wild', 'well', 'times', 'sinking', 'oil', 'fucking', 'check', 'cause', 'weapons', 'truck', 'food', 'bloody', 'always', 'weapon', 'theres', 'state', 'little', 'injuries', 'free', 'wounded', 'summer', 'smoke', 'severe', 'reddit', 'next', 'movie', 'ive', 'hes', 'fall', 'evacuate', 'confirmed', 'bad', 'again', 'thunderstorm', 'set', 'night', 'natural', 'looks', 'heat', 'face', 'earthquake', 'boy', 'whole', 'until', 'thunder', 'through', 'says', 'panic', 'outbreak', 'made', 'lightning', 'fatalities', 'family', 'explosion', 'end', 'destroy', 'derailment', 'air', 'w', 'terrorist', 'survive', 'screaming', 'saudi', 'refugees', 'rain', 'murder', 'loud', 'liked', 'house', 'gonna', 'failure', 'collided', 'bag', 'attacked', 'ambulance', '70', 'wind', 'services', 'save', 'report', 'migrants', 'head', 'explode', 'charged', 'change', 'big', 'also', 'wrecked', 'warning', 'update', 'run', 'rescuers', 'released', 'photo', 'massacre', 'injury', 'hurricane', 'high', 'hail', 'fuck', 'does', 'destroyed', 'bus', 'blood', '40', '\\x89ÛÒ', 'wreckage', 'violent', 'twister', 'trauma', 'tragedy', 'terrorism', 'survivors', 'survived', 'sinkhole', 'sandstorm', 'road', 'rioting', 'red', 'real', 'put', 'post', 'national', 'missing', 'landslide', 'keep', 'girl', 'drought', 'curfew', 'breaking', 'bags', 'white', 'twitter', 'tonight', 'structural', 'spill', 'service', 'screamed', 'rescued', 'rescue', 'phone', 'ok', 'oh', 'mosque', 'lives', 'horrible', 'harm', 'game', 'dust', 'destruction', 'deluge', 'deaths', 'crashed', 'cliff', 'catastrophe', 'boat', 'away', 'august', 'area', 'apocalypse', 'woman', 'whirlwind', 'traumatised', 'stock', 'saw', 'ruin', 'riot', 'quarantine', 'kills', 'island', 'investigators', 'ill', 'hostages', 'hazard', 'danger', 'call', '15', 'women', 'windstorm', 'things', 'suspect', 'show', 'reunion', 'quarantined', 'lava', 'heart', 'engulfed', 'detonate', 'crush', 'collapsed', 'came', 'better', 'battle', 'armageddon', 'airplane', 'against', 'affected', 'use', 'trapped', 'thank', 'sunk', 'story', 'send', 'part', 'other', 'must', 'mudslide', 'market', 'iran', 'famine', 'exploded', 'electrocuted', 'ebay', 'displaced', 'derailed', 'derail', 'burned', 'bombed', 'blown', 'baby', 'around', 'zone', 'wave', 'wanna', 'sure', 'someone', 'screams', 'razed', 'power', 'obliterated', 'long', 'land', 'hundreds', 'heard', 'group', 'flattened', 'drown', 'doing', 'care', 'bridge', 'bagging', '9', 'went', 'used', 'typhoon', 'trouble', 'tornado', 'thought', 'thing', 'river', 'responders', 'past', 'pandemonium', 'officials', 'meltdown', 'lot', 'least', 'inundated', 'id', 'hostage', 'hijacking', 'hazardous', 'goes', 'drowning', 'didnt', 'devastation', 'demolish', 'collide', 'casualties', 'calgary', 'bang', 'anniversary', 'yet', 'wounds', 'volcano', 'tsunami', 'sue', 'st', 'song', 'something', 'shoulder', 'security', 'prebreak', 'possible', 'pkk', 'panicking', 'obliteration', 'obliterate', 'murderer', 'minute', 'light', 'lets', 'kill', 'isis', 'india', 'hijacker', 'hellfire', 'government', 'few', 'evacuated', 'due', 'detonated', 'desolation', 'crushed', 'chemical', 'blew', 'blazing', 'blast', 'annihilated', 'airport', '6', 'week', 'upheaval', 'trying', 'three', 'thanks', 'sound', 'soon', 'sirens', 'rainstorm', 'plane', 'music', 'making', 'kids', 'issues', 'half', 'guys', 'fedex', 'done', 'died', 'detonation', 'days', 'cyclone', 'county', 'collision', 'caused', 'catastrophic', 'bleeding', 'beautiful', '8', 'words', 'very', 'traffic', 'south', 'remember', 'policy', 'place', 'nothing', 'north', 'mp', 'longer', 'left', 'israeli', 'hell', 'fun', 'drowned', 'demolished', 'cool', 'both', 'bioterror', 'believe', 'avalanche', 'arson', 'turkey', 'snowstorm', 'site', 'shot', 'shooting', 'pic', 'nowplaying', 'media', 'islam', 'inside', 'hijack', 'helicopter', 'fight', 'fatality', 'fan', 'electrocute', 'doesnt', 'building', 'brown', 'bc', 'actually', '16yr', 'yes', 'watching', 'wait', 'ur', 'tell', 'swallowed', 'seismic', 'second', 'rubble', 're\\x89Û', 'plans', 'men', 'memories', 'line', 'la', 'horror', 'health', 'having', 'find', 'eyewitness', 'deluged', 'children', 'bush', 'anything', 'already', 'almost', 'aircraft', 'yourself', 'yeah', 'whats', 'tomorrow', 'such', 'start', 'side', 'searching', 'saved', 'reactor', 'probably', 'play', 'person', 'peace', 'outside', 'officer', 'nearby', 'n', 'maybe', 'lost', 'literally', 'hours', 'hear', 'far', 'die', 'demolition', 'data', 'crews', 'conclusively', 'business', 'american', '20', '\\x89ÛÓ', 'west', 'waves', 'team', 'street', 'stay', 'soudelor', 'reuters', 'manslaughter', 'leather', 'job', 'history', 'hey', 'feeling', 'eyes', 'everything', 'declares', 'deal', 'casualty', 'bodies', 'amid', 'ablaze', '7', '50', '30', '12', 'youth', 'wont', 'wake', 'theyre', 'support', 'stretcher', 'same', 'rise', 'picking', 'photos', 'own', 'others', 'order', 'omg', 'okay', 'name', 'myself', 'money', 'makes', 'leave', 'lab', 'gt', 'gets', 'flag', 'desolate', 'crisis', 'center', 'book', 'blight', 'blaze', 'ago', 'abc', '11yearold', 'womens', 'typhoondevastated', 'tv', 'trench', 'trains', 'texas', 'space', 'siren', 'shes', 'self', 'saipan', 'reason', 'rd', 'pretty', 'pick', 'offensive', 'move', 'meek', 'major', 'm', 'low', 'lord', 'huge', 'hat', 'flash', 'feared', 'fast', 'effect', 'course', 'country', 'control', 'class', 'child', 'chance', 'caught', 'called', 'bioterrorism', 'bestnaijamade', 'become', 'bar', 'banned', 'ball', 'aug', 'annihilation', 'wrong', 'win', 'usa', 'united', 'town', 'totally', 'toddler', 'though', 'temple', 'taken', 'stand', 'spot', 'signs', 'ship', 'pakistan', 'online', 'level', 'ladies', 'jobs', 'isnt', 'happy', 'hailstorm', 'friends', 'disea', 'damn', 'couple', 'case', 'blue', 'bigger', 'america', 'across', '10', 'yours', 'village', 'try', 'transport', 'talk', 'seen', 'russian', 'radio', 'projected', 'once', 'official', 'needs', 'nearly', 'mount', 'might', 'mayhem', 'instead', 'hollywood', 'haha', 'guy', 'gun', 'green', 'front', 'finally', 'favorite', 'experts', 'entire', 'east', 'daily', 'crazy', 'computers', 'coaches', 'christian', 'china', 'blizzard', 'anyone', 'aint', 'action', '25', 'virgin', 'vehicle', 'truth', 'trust', 'takes', 't', 'star', 'sorry', 'running', 'refugio', 'reddits', 'poor', 'pain', 'mom', 'miners', 'marks', 'looking', 'knock', 'issued', 'insurance', 'ignition', 'houses', 'heavy', 'hate', 'hard', 'happened', 'global', 'giant', 'gbbo', 'flight', 'eye', 'emmerdale', 'driver', 'devastated', 'd', 'costlier', 'cnn', 'cars', 'camp', 'beach', 'arsonist', 'angry', 'alone', 'added', '05', 'york', 'wonder', 'uk', 'turn', 'taking', 'subreddits', 'sounds', 'scared', 'russia', 'rly', 'reports', 'ready', 'quiz', 'public', 'property', 'pradesh', 'ppl', 'playing', 'pay', 'parole', 'pamela', 'pakistani', 'outrage', 'niggas', 'nagasaki', 'myanmar', 'muslims', 'mop', 'madhya', 'mad', 'lmao', 'learn', 'large', 'govt', 'give', 'gems', 'gave', 'funtenna', 'fukushima', 'former', 'film', 'earth', 'drive', 'downtown', 'dog', 'comes', 'closed', 'cake', 'british', 'bring', 'bbc', 'b', 'appears', 'aftershock', '13', '11', 'young', 'wow', 'worst', 'waving', 'washington', 'wanted', 'vs', 'view', 'upon', 'tweet', 'tree', 'tote', 'thousands', 'thinking', 'theater', 'soul', 'sky', 'sign', 'shows', 'shift', 'seeing', 'sea', 'scene', 'safety', 'rules', 'rock', 'reported', 'r', 'pray', 'playlist', 'patience', 'passengers', 'park', 'nws', 'nigerian', 'morning', 'moment', 'mode', 'listen', 'likely', 'libya', 'led', 'israel', 'human', 'hiring', 'handbag', 'hand', 'gop', 'geller', 'gas', 'galactic', 'friend', 'france', 'following', 'follow', 'fashion', 'enough', 'else', 'driving', 'drake', 'don\\x89Ûªt', 'disease', 'declaration', 'cut', 'colorado', 'chinas', 'chile', 'centre', 'businesses', 'biggest', 'behind', 'bed', 'bayelsa', 'awesome', 'arrested', 'apollo', 'ancient', '70th', '100', 'x', 'working', 'wednesday', 'wasnt', 'unconfirmed', 'twelve', 'turned', 'tried', 'thursday', 'terror', 'super', 'sex', 'secret', 'sad', 'risk', 'recount', 'problem', 'potus', 'planned', 'parents', 'occurred', 'number', 'neighbours', 'mph', 'militants', 'middle', 'link', 'landing', 'lamp', 'info', 'idea', 'holding', 'governor', 'glad', 'germs', 'firefighters', 'escape', 'ebola', 'early', 'dude', 'deep', 'date', 'cree', 'claims', 'cdt', 'break', 'ave', 'art', 'anthrax', '60', '16', '12000', 'worse', 'worlds', 'western', 'wants', 'wall', 'walk', 'using', 'true', 'till', 'started', 'spring', 'small', 'silver', 'serious', 'provoke', 'played', 'party', 'nice', 'mountain', 'months', 'mod', 'miss', 'mishaps', 'metal', 'med', 'lies', 'lake', 'lady', 'japanese', 'involving', 'investigating', 'internet', 'hour', 'helping', 'havent', 'gtgt', 'funny', 'force', 'financial', 'faux', 'falling', 'expected', 'download', 'direction', 'department', 'denver', 'delivers', 'crematoria', 'coast', 'climate', 'ca', 'buy', 'bombs', 'album', 'alarm', 'ahead', 'account', 'aba', '\\x89ÛÏwhen', 'youll', 'wish', 'wedding', 'victims', 'udhampur', 'turkish', 'travel', 'took', 'told', 'together', 'todays', 'threat', 'sun', 'stories', 'sick', 'shots', 'share', 'series', 'sense', 'season', 'saying', 'salt', 'room', 'record', 'purse', 'prepare', 'plunging', 'philippines', 'pass', 'p', 'offroad', 'nigga', 'members', 'means', 'louis', 'london', 'living', 'linked', 'king', 'join', 'international', 'information', 'horse', 'hasnt', 'happening', 'guide', 'guess', 'ground', 'future', 'friday', 'four', 'feat', 'fact', 'expect', 'etc', 'door', 'different', 'dies', 'de', 'company', 'comment', 'causes', 'career', 'cameroon', 'begin', 'australia', 'arent', 'answer', 'along', 'allows', 'alabama', 'airlines', 'act', '600', '2014', '17', 'wtf', 'worry', 'wife', 'weekend', 'walking', 'victim', 'v', 'utc20150805', 'trump', 'tho', 'text', 'test', 'teen', 'taiwan', 'tablet', 'syrian', 'strike', 'straight', 'states', 'short', 'ships', 'shape', 'setting', 'sensorsenso', 'search', 'rocky', 'research', 'reading', 'rather', 'rate', 'press', 'piece', 'phoenix', 'orders', 'ones', 'nyc', 'minutes', 'million', 'michael', 'mean', 'map', 'lucky', 'killing', 'jonathan', 'islamic', 'internally', 'interesting', 'indian', 'hero', 'happens', 'happen', 'hair', 'giving', 'girls', 'general', 'freak', 'fighting', 'felt', 'feels', 'fat', 'fans', 'episode', 'enugu', 'effects', 'each', 'e', 'drunk', 'double', 'dogs', 'destroys', 'desires', 'dc', 'dance', 'dan', 'cover', 'court', 'complete', 'cold', 'close', 'changes', 'carrying', 'captures', 'cable', 'buses', 'brooklyn', 'board', 'blocked', 'bit', 'between', 'ban', 'ashes', 'arianagrande', 'apc', 'animal', 'amazon', 'alive', 'agree', 'africa', 'absolutely', '2nd', '24', '2013', '1st', '19', '18', '0', 'yall', 'wrought', 'worth', 'winds', 'vote', 'version', 'upset', 'un', 'trent', 'trees', 'training', 'tracks', 'total', 'technology', 'suspected', 'strong', 'standard', 'stadium', 'specially', 'special', 'sometimes', 'soldiers', 'snow', 'smaug', 'sleeping', 'single', 'sh', 'selfimage', 'route', 'ross', 'roosevelt', 'response', 'repatriated', 'remove', 'radiation', 'queen', 'program', 'prevent', 'population', 'picture', 'petition', 'palestinian', 'office', 'nc', 'navy', 'moving', 'millions', 'metro', 'meet', 'mediterranean', 'marians', 'leaving', 'lead', 'late', 'kit', 'killer', 'image', 'hold', 'hobo', 'hello', 'handbags', 'gunman', 'grows', 'gotta', 'gone', 'gold', 'games', 'fully', 'french', 'foxnews', 'feet', 'express', 'examining', 'event', 'evening', 'either', 'dr', 'dangerous', 'dad', 'currently', 'cranes', 'cops', 'colour', 'chicago', 'central', 'causing', 'cat', 'cannot', 'canada', 'calls', 'brother', 'broke', 'box', 'beyond', 'beat', 'based', 'avoid', 'anymore', 'afghanistan', 'able', 'abandoned', '22', '1980', '14', '06', 'yyc', 'wwii', 'word', 'won', 'woke', 'videos', 'usgs', 'usatoday', 'updates', 'tribal', 'trfc', 'totaling', 'thunderstorms', 'tension', 'tears', 'target', 'talking', 'supposed', 'sunday', 'stuff', 'students', 'struggles', 'starts', 'stage', 'squad', 'spos', 'spaceship', 'son', 'sitting', 'sismo', 'shut', 'shouldnt', 'shall', 'seeks', 'seek', 'screen', 'saving', 'san', 'runs', 'relief', 'region', 'reasons', 're', 'question', 'president', 'point', 'pile', 'perfect', 'percent', 'pdp', 'paul', 'open', 'municipal', 'motorcyclist', 'month', 'modified', 'match', 'main', 'lots', 'lose', 'lie', 'less', 'leading', 'later', 'kinda', 'keeps', 'insurer', 'industry', 'imagine', 'ice', 'hunt', 'hospital', 'honestly', 'hilarious', 'ft', 'forever', 'forecast', 'football', 'fleets', 'five', 'fears', 'extreme', 'everywhere', 'emotional', 'drink', 'dream', 'damaged', 'da', 'cruz', 'crime', 'create', 'cook', 'confirms', 'completely', 'community', 'combo', 'coffee', 'cancer', 'c130', 'brought', 'broken', 'boys', 'books', 'block', 'began', 'bay', 'babies', 'attacks', 'ask', 'apply', 'ap', 'antioch', 'among', 'allah', 'according', '911', '4000', '31', '2011', 'yo', 'worldnews', 'wmata', 'wired', 'welcome', 'watched', 'wars', 'waiting', 'usually', 'units', 'understand', 'tropical', 'tram', 'tonto', 'tips', 'time20150806', 'tickets', 'thx', 'terrorists', 'system', 'streets', 'stream', 'stopped', 'sports', 'spent', 'sir', 'simple', 'signed', 'shower', 'shares', 'shame', 'sent', 'seems', 'secrets', 'seconds', 'seattle', 'saturday', 'satellite', 'rule', 'round', 'rip', 'ridge', 'richmond', 'review', 'return', 'repair', 'rea\\x89Û', 'quite', 'pressure', 'practice', 'potential', 'pool', 'plus', 'pilot', 'pics', 'parenthood', 'owner', 'outlook', 'original', 'opens', 'opening', 'onto', 'oklahoma', 'ocean', 'o784', 'o', 'nursing', 'nah', 'mortal', 'mix', 'mine', 'mention', 'mediterran', 'matter', 'massive', 'mary', 'lt', 'loving', 'loved', 'list', 'limited', 'lil', 'leads', 'leader', 'kombat', 'kid', 'kick', 'justinbieber', 'issue', 'irandeal', 'inner', 'incident', 'impact', 'humanity', 'hobbit', 'hits', 'himself', 'highway', 'hi', 'heres', 'hawaii', 'hands', 'guns', 'grill', 'greatest', 'gov', 'gm', 'gay', 'forget', 'firefighter', 'fine', 'final', 'false', 'facebook', 'faan', 'exploration', 'exit', 'exchanging', 'epic', 'el', 'edm', 'earlier', 'drinking', 'disco', 'directioners', 'deadly', 'david', 'dark', 'crying', 'cost', 'continue', 'concerned', 'companies', 'common', 'civilians', 'civilian', 'character', 'cases', 'careful', 'canyon', 'calling', 'burn', 'build', 'browser', 'bro', 'bringing', 'bought', 'blog', 'birthday', 'bin', 'bicyclist', 'ben', 'battlefield', 'arrived', 'ar', 'amazing', 'afternoon', 'acres', 'accidents', 'abstorm', '90', '500', '26', 'you\\x89Ûªve', 'youve', 'yesterday', 'wx', 'writing', 'worked', 'within', 'wildfires', 'weird', 'ways', 'warnings', 'wa', 'vuitton', 'voice', 'visit', 'vietnam', 'unless', 'universe', 'trip', 'township', 'tongue', 'toilet', 'thomas', 'tent', 'tank', 'syria', 'survival', 'stupid', 'stone', 'steve', 'starting', 'sport', 'somalia', 'social', 'smh', 'sleep', 'situation', 'shoes', 'seven', 'sensor', 'sees', 'seat', 'science', 'sadly', 'rubber', 'roof', 'roads', 'result', 'respond', 'replace', 'remains', 'related', 'reduce', 'recovery', 'realise', 'rare', 'rail', 'quote', 'quickly', 'quick', 'pussy', 'protect', 'problems', 'price', 'preparedness', 'powerful', 'posts', 'portland', 'pop', 'political', 'points', 'player', 'photography', 'pathogens', 'officers', 'none', 'needed', 'nature', 'mumbai', 'ms', 'mr', 'moments', 'missed', 'minecraft', 'mill', 'mile', 'md', 'mayan', 'manchester', 'malaysian', 'loss', 'loose', 'listening', 'lion', 'linkury', 'lights', 'lifted', 'laden', 'knew', 'kindle', 'jeb', 'it\\x89Ûªs', 'italian', 'iraq', 'invoices', 'inundation', 'inj', 'including', 'illegal', 'i5', 'hwy', 'hwo', 'httptcoqew4c5m1xd', 'hill', 'hearing', 'headed', 'ha', 'guardian', 'gods', 'glass', 'genocide', 'gaza', 'freakiest', 'fort', 'forces', 'fog', 'fish', 'feminists', 'fell', 'fantasy', 'falls', 'f', 'experience', 'exchange', 'except', 'equipment', 'epicentre', 'ep', 'ended', 'em', 'economy', 'economic', 'dropped', 'dozens', 'disney', 'discovered', 'di', 'created', 'continues', 'conference', 'computer', 'clutch', 'church', 'choice', 'china\\x89Ûªs', 'chief', 'cheese', 'charity', 'canaanites', 'burns', 'bruh', 'broadway', 'brain', 'bout', 'bombings', 'beyhive', 'bet', 'baseball', 'bank', 'automatic', 'audio', 'attention', 'asking', 'asked', 'articles', 'areas', 'arabia', 'angel', 'al', 'ah', 'add', 'access', '4x4', '3g', '1000', 'zombie', 'yrs', 'ya', 'xd', 'write', 'wouldnt', 'wins', 'wings', 'window', 'wheavenly', 'weeks', 'wed', 'waste', 'vinyl', 'vine', 'veterans', 'van', 'valley', 'utc', 'updated', 'unlocked', 'unit', 'type', 'twice', 'tweets', 'tube', 'troops', 'triggered', 'trailer', 'tour', 'tough', 'touch', 'toronto', 'tom', 'toll', 'tired', 'threatens', 'thoughts', 'theatre', 'terrible', 'ten', 'tcot', 'swallows', 'sw', 'surrounded', 'surprise', 'sunset', 'subject', 'style', 'struck', 'storms', 'store', 'stops', 'steps', 'steel', 'stealing', 'stars', 'standuser', 'sparked', 'soundcloud', 'somebody', 'society', 'snap', 'slightly', 'skin', 'size', 'sit', 'sinjar', 'sidelines', 'sicily', 'shelter', 'shadow', 'several', 'september', 'senior', 'senator', 'se', 'scary', 'satchel', 'santa', 'salem', 'sale', 'safe', 'runway', 'row', 'root', 'rocks', 'rn', 'rising', 'riots', 'residents', 'republicans', 'reminds', 'remembering', 'releases', 'release', 'relatives', 'ran', 'quran', 'questions', 'putin', 'prosecuted', 'prophetmuhammad', 'project', 'prime', 'powerlines', 'poll', 'pipe', 'pilots', 'pc', 'patient', 'passing', 'parleys', 'parker', 'page', 'operation', 'officially', 'nytimes', 'ny', 'notices', 'nine', 'nashville', 'murdered', 'mt', 'movies', 'moon', 'mood', 'monogram', 'mma', 'mission', 'minister', 'mining', 'mini', 'mind', 'mikeparractor', 'mets', 'message', 'mentions', 'meeting', 'medical', 'meatloving', 'maximum', 'max', 'markets', 'maintenance', 'madinah', 'mac', 'lt3', 'lovely', 'loop', 'local', 'library', 'levels', 'legacy', 'learning', 'laws', 'lane', 'lack', 'knows', 'knee', 'kisii', 'kidnapped', 'john', 'joe', 'jackson', 'jack', 'i\\x89Ûªm', 'itunes', 'interview', 'intensity', 'indeed', 'incredible', 'ii', 'ie', 'ideas', 'hurts', 'hunters', 'humans', 'httptcoq2eblokeve', 'httptcoencmhz6y34', 'homeless', 'heroes', 'held', 'heads', 'h', 'gunfire', 'gta', 'gotten', 'given', 'girlfriend', 'german', 'fruit', 'fr', 'foxtrot', 'forced', 'followers', 'focus', 'florida', 'floor', 'flat', 'fix', 'fired', 'female', 'feed', 'failed', 'fail', 'exp', 'executives', 'evil', 'everyday', 'estimate', 'entertainment', 'enjoy', 'england', 'enemy', 'electronic', 'edt', 'edition', 'eat', 'dying', 'dubstep', 'drone', 'drill', 'dollar', 'documents', 'dnb', 'distance', 'destiny', 'defense', 'debate', 'cuz', 'cute', 'curved', 'criminals', 'crap', 'cramer', 'couldnt', 'costs', 'cop', 'consider', 'congress', 'conditions', 'concert', 'complex', 'co', 'club', 'click', 'chinese', 'charging', 'ceo', 'carry', 'card', 'capsizes', 'can\\x89Ûªt', 'calories', 'calm', 'calif', 'c', 'by\\x89Û', 'button', 'butter', 'burst', 'budget', 'broad', 'brazil', 'boston', 'blvd', 'blk', 'blessings', 'blessed', 'blamed', 'bells', 'bear', 'awful', 'avoiding', 'authorities', 'australian', 'associated', 'arrive', 'arms', 'apparently', 'app', 'animalrescue', 'amsterdam', 'aka', 'aim', 'af', 'advisory', 'adult', 'actions', 'accidentally', 'above', '5km', '53inch', '4wd', '35', '300w', '1945', 'zionist', 'z10', 'youngheroesid', 'youd', 'yobe', 'yay', 'xbox', 'ww1', 'written', 'wouldve', 'worried', 'workers', 'wondering', 'wonderful', 'wo', 'williams', 'whos', 'whether', 'wet', 'weed', 'wearing', 'wealth', 'warship', 'warns', 'viralspell', 'violence', 'vehicles', 'usual', 'users', 'upper', 'unveiled', 'ultimate', 'ugh', 'twia', 'tryna', 'truly', 'treatment', 'treat', 'trapmusic', 'trap', 'traditional', 'track', 'towel', 'toward', 'tornadoes', 'title', 'thus', 'throwing', 'thriller', 'threatening', 'theyll', 'tennessee', 'tempered', 'ted', 'tech', 'teams', 'tea', 'tbt', 'tax', 'tag', 'sydney', 'swim', 'sweet', 'surprised', 'stuart', 'structures', 'structure', 'streak', 'strategy', 'strange', 'stranded', 'status', 'statement', 'staff', 'springs', 'spirit', 'spider', 'spanish', 'somehow', 'solution', 'solar', 'socialnews', 'smoking', 'sister', 'singing', 'sing', 'shop', 'shipping', 'shepherd', 'sharp', 'sexual', 'served', 'seriously', 'serial', 'sentinel', 'sarah', 'sac', 'rÌ©union', 'romance', 'roll', 'revealed', 'results', 'restore', 'rest', 'responsible', 'responds', 'respect', 'residential', 'reporting', 'reno', 'remain', 'register', 'reduced', 'rear', 'realized', 'read\\x89Û', 'ray', 'rape', 'rally', 'rains', 'quest', 'quality', 'putting', 'purple', 'pump', 'protector', 'prophet', 'profile', 'prices', 'pres', 'pov', 'port', 'politics', 'plug', 'plot', 'playoffs', 'planet', 'plains', 'plague', 'places', 'piling', 'picked', 'physical', 'philly', 'persons', 'personal', 'peanut', 'patrick', 'parts', 'paper', 'pantherattack', 'pak', 'owners', 'option', 'okwx', 'of\\x89Û', 'offer', 'numbers', 'nuke', 'normal', 'newest', 'nd', 'naved', 'nation', 'nasahurricane', 'named', 'museum', 'murderous', 'moves', 'moved', 'motor', 'mo', 'mlb', 'miles', 'migrant', 'mido', 'microlight', 'mhtw4fnet', 'mess', 'memory', 'memorial', 'maria', 'mansehra', 'manager', 'management', 'mail', 'lungs', 'luck', 'lowndes', 'lower', 'loves', 'lorries', 'looting', 'looked', 'let\\x89Ûªs', 'legal', 'law', 'laughing', 'larger', 'justice', 'july', 'judge', 'jewish', 'jet', 'jamaica', 'jam', 'jacksonville', 'itself', 'italy', 'ir', 'iphone', 'insane', 'injuryi495', 'increased', 'iii', 'igers', 'ices\\x89Û', 'icemoon', 'i77', 'hungry', 'httptcovvplfqv58p', 'httptcoksawlyux02', 'housing', 'hotel', 'hopefully', 'hip', 'hearts', 'hd', 'guillermo', 'gtgtgt', 'grow', 'grey', 'grade', 'grace', 'google', 'golf', 'goal', 'germany', 'george', 'generation', 'gang', 'gabon', 'fresh', 'freedom', 'fox', 'finnish', 'fighters', 'field', 'festival', 'fav', 'fake', 'faced', 'extremely', 'extra', 'external', 'explosionproof', 'experiments', 'exactly', 'everyones', 'estimated', 'especially', 'eruption', 'error', 'energy', 'ends', 'ems', 'elephant', 'electrical', 'effort', 'education', 'easy', 'dvd', 'duty', 'dutch', 'drug', 'drop', 'drones', 'drawn', 'dramatic', 'djicemoon', 'diving', 'disrupts', 'disneys', 'diet', 'details', 'depth', 'demand', 'degrees', 'decided', 'debt', 'deals', 'darude', 'dare', 'damages', 'cry', 'crossed', 'cream', 'covers', 'covered', 'countries', 'copilot', 'considering', 'conflict', 'confirm', 'combined', 'collisionno', 'collection', 'clip', 'clear', 'clean', 'classic', 'civil', 'cities', 'cinema', 'cia', 'christmas', 'christians', 'charge', 'characters', 'certain', 'campaign', 'built', 'buffalo', 'bride', 'brakes', 'bowl', 'boss', 'bob', 'blind', 'blackberry', 'bjp', 'bitch', 'be\\x89Û', 'below', 'beginning', 'becomes', 'became', 'beam', 'bb17', 'bat', 'bargain', 'b4', 'australias', 'asap', 'article', 'approaches', 'anyway', 'anti', 'annual', 'animals', 'anchorage', 'amongst', 'americans', 'alps', 'allow', 'aid', 'age', 'afterlife', 'afraid', 'afghan', 'advance', 'address', 'acts', 'activity', 'activated', 'abuse', 'abcnews', '97georgia', '731', '3d', '33', '320', '300', '101', 'åÊ', '\\x89Û÷politics', '\\x89ÛÏthe', '\\x89ÛÏa', 'zouma', 'z', 'yr', 'yazidis', 'y', 'xp', 'wy', 'writer', 'wound', 'worstsummerjob', 'wood', 'wither', 'wit', 'wireless', 'wire', 'winston', 'windows', 'wheres', 'whao', 'weve', 'werent', 'website', 'wash', 'warfighting', 'wanting', 'walmart', 'waimate', 'vulnerable', 'votejkt48id', 'vintage', 'vets', 'values', 'val', 'vacant', 'va', 'utterly', 'utter', 'uses', 'usagov', 'uribe', 'unsuckdcmetro', 'unlocking', 'university', 'union', 'understanding', 'ugly', 'uganda', 'twin', 'tutorial', 'turns', 'turbine', 'tubestrike', 'trucks', 'trolley', 'tripledigit', 'tribune', 'trial', 'transit', 'tracking', 'tools', 'tons', 'todd', 'time20150805', 'throw', 'thousand', 'theyve', 'theory', 'themselves', 'th', 'temporary300', 'telling', 'taste', 'targeting', 'tampa', 'table', 'swimming', 'swansea', 'supreme', 'summerfate', 'suffer', 'sucks', 'success', 'subs', 'sub', 'study', 'stress', 'strategicpatience', 'stir', 'stereo', 'station', 'starter', 'stands', 'stabbing', 'stab', 'sprinter', 'spend', 'speaking', 'speaker', 'sparks', 'spain', 'sp', 'southern', 'southeast', 'southampton', 'sources', 'source', 'soup', 'sophie', 'someones', 'smithsonian', 'smile', 'slow', 'slide', 'slanglucci', 'skinny', 'sixth', 'six', 'sisters', 'simulate', 'silence', 'shell', 'sets', 'senate', 'selfie', 'section', 'sean', 'scott', 'score', 'scientists', 'schools', 'scheme', 'schedule', 'sat', 'sandiego', 'sanctions', 'safer', 'sa', 'rs', 'rose', 'rolling', 'rohingya', 'rockyfire', 'rockin', 'rocket', 'robots', 'robert', 'rob', 'rises', 'ring', 'rights', 'ridiculous', 'rider', 'ride', 'richard', 'reward', 'reveals', 'responsibility', 'republican', 'reportedly', 'replacing', 'remembered', 'regular', 'refugee', 'redlight', 'recommend', 'recognize', 'recently', 'recent', 'recall', 'reap', 'realize', 'reality', 'reaching', 'reach', 'rapidly', 'raining', 'rage', 'radar', 'racist', 'race', 'pulls', 'pulled', 'ps', 'providence', 'protest', 'prompts', 'promises', 'promise', 'progress', 'produced', 'probe', 'pro', 'prison', 'prior', 'print', 'prince', 'primary', 'previously', 'prepared', 'pregnant', 'predicted', 'predict', 'precipitation', 'prabhu', 'poverty', 'posted', 'positive', 'pls', 'players', 'plate', 'planted', 'plant', 'plaguing', 'pitch', 'pipeline', 'photoshop', 'phones', 'perhaps', 'pepper', 'pbban', 'path', 'passenger', 'parking', 'palms', 'palestinians', 'palestine', 'painting', 'packs', 'pacific', 'pace', 'overnight', 'outdoor', 'otrametlife', 'oppressions', 'opposite', 'opinion', 'opened', 'oops', 'omfg', 'olympic', 'okanagan', 'often', 'offers', 'occasion', 'nw', 'nurse', 'nu', 'np', 'notice', 'nobody', 'nigeria', 'nicki', 'newyork', 'newlyweds', 'network', 'nema', 'negative', 'nb', 'nasa', 'mystery', 'muslim', 'muscle', 'multiple', 'mtvhottest', 'mourning', 'mountains', 'motorcycle', 'mother', 'morgan', 'monsoon', 'minds', 'min', 'michigan', 'michael5sos', 'miami', 'mexico', 'messenger', 'mens', 'memes', 'measurement', 'meant', 'meals', 'materials', 'maryland', 'marked', 'mark', 'mansion', 'mama', 'mall', 'majority', 'magic', 'machine', 'lunch', 'lulgzimbestpicts', 'lowly', 'losses', 'losing', 'los', 'longest', 'lonely', 'logo', 'location', 'loan', 'load', 'lmfao', 'listed', 'lip', 'lightening', 'lez', 'leveled', 'letting', 'letters', 'lethal', 'legit', 'leaves', 'league', 'leaders', 'largest', 'kyle', 'known', 'kind', 'kerricktrial', 'kept', 'kca', 'karymsky', 'jst', 'joy', 'jordan', 'jeans', 'japÌn', 'i\\x89Û', 'iranian', 'invalid', 'internal', 'intern', 'interested', 'interest', 'insurers', 'instagram', 'innocent', 'include', 'impossible', 'images', 'ig', 'idp', 'idk', 'idfire', 'hurt', 'huh', 'hughes', 'huffman', 'httptcozujwuiomb3', 'httptcowvj39a3bgm', 'httptcoviwxy1xdyk', 'httptcothoyhrhkfj', 'httptcolvlh3w3awo', 'httpstcomoll5vd8yd', 'hosting', 'host', 'horses', 'horrific', 'hop', 'honors', 'holy', 'holland', 'holiday', 'hole', 'hitting', 'hills', 'highest', 'hieroglyphics', 'here\\x89Ûªs', 'heights', 'heaven', 'headphones', 'heading', 'haunting', 'hated', 'harry', 'harbor', 'haram', 'gusty', 'gusts', 'groups', 'grief\\x89Ûª', 'grenade', 'grazed', 'grateful', 'grab', 'gift', 'garden', 'gained', 'g', 'fwy', 'further', 'fund', 'fuel', 'fucked', 'frontline', 'forward', 'forgotten', 'forgot', 'forbes', 'follows', 'fly', 'floyds', 'flooded', 'flew', 'flags', 'fits', 'fingers', 'finds', 'fights', 'fifth', 'fettilootch', 'feeding', 'fbi', 'father', 'fate', 'faster', 'farrakhan', 'fantastic', 'famous', 'falcon', 'faith', 'factory', 'explains', 'expert', 'exist', 'ex', 'eventually', 'events', 'europe', 'estate', 'equal', 'epilepsy', 'entered', 'enter', 'enjoying', 'english', 'emotions', 'elevated', 'elem', 'electric', 'election', 'efforts', 'edinburgh', 'eden', 'ebike', 'eating', 'easily', 'earrings', 'earners', 'dtn', 'dry', 'drugs', 'drove', 'driven', 'dress', 'dragon', 'doubt', 'dk', 'diss', 'dinner', 'diamond', 'detectado', 'desire', 'designs', 'design', 'description', 'derails', 'deputies', 'dependency', 'democracy', 'delays', 'degree', 'defend', 'decisions', 'decision', 'decide', 'dear', 'daughters', 'daughter', 'dam', 'dallas', 'cyclist', 'current', 'cupcake', 'cup', 'crowd', 'crater', 'crashes', 'cr', 'cousin', 'couples', 'counselor', 'cotton', 'cos', 'cord', 'copycat', 'constantly', 'concerns', 'concern', 'compliant', 'communities', 'commercial', 'comments', 'collapses', 'cofounder', 'cock', 'clouds', 'cloud', 'clinton', 'client', 'cleveland', 'clev', 'clearly', 'cleanup', 'clash', 'claim', 'chill', 'childhood', 'chicagoarea', 'chelsea', 'charts', 'charlie', 'charles', 'channel', 'challenge', 'certainly', 'cell', 'cdc', 'caution', 'cats', 'catch', 'cast', 'carryi', 'captured', 'capture', 'captain', 'capacity', 'cancel', 'campus', 'californias', 'calamity', 'bulletin', 'bug', 'buckle', 'brutally', 'brothers', 'brings', 'braking', 'boxer', 'bound', 'boot', 'bluetooth', 'bluejays', 'blows', 'blocking', 'bless', 'bitches', 'bid', 'bff', 'beyonce', 'benefits', 'bell', 'begins', 'bees', 'beer', 'bedroom', 'beats', 'bears', 'battling', 'battery', 'bathroom', 'base', 'bands', 'bake', 'averted', 'average', 'av', 'auth', 'aussie', 'auctions', 'attacking', 'atlanta', 'assistance', 'asian', 'asia', 'arabian', 'apple', 'anybody', 'answers', 'ankle', 'angeles', 'analysis', 'america\\x89Ûªs', 'americas', 'allowed', 'alleged', 'alert', 'alcohol', 'alberta', 'alaska', 'agreed', 'admits', 'active', 'activates', 'accuses', 'ability', 'abbswinston', '852015', '80s', '800', '6aug', '5pm', '5000', '48', '3rd', '39', '370', '360wisenews', '34', '2pm', '2pcs', '29072015', '23', '21', '2030', '2009', '18w', '13000', '125', '1200', '10th', '02', 'å¨', '\\x89Û÷extremely', '\\x89ÛÏwe', '\\x89ÛÏrichmond', '\\x89ÛÏhatchet', 'zayn', 'yugvani', 'yorker', 'yep', 'yemen', 'yea', 'yahoonews', 'yahoo', 'xxx', 'wwi', 'wrapup', 'worldnetdaily', 'wom', 'wnd', 'witter', 'winter', 'willing', 'widespread', 'wicked', 'wi', 'wht', 'whales', 'weight', 'wee', 'weathernetwork', 'wear', 'weak', 'waterways', 'waterresistant', 'washed', 'warships', 'warned', 'warne', 'warn', 'warcraft', 'wannabe', 'walls', 'walker', 'voter', 'volga', 'vladimir', 'visited', 'virus', 'virgil', 'vip', 'villages', 'villagers', 'vid', 'vet', 'vest', 'vermont', 'verdict', 'venezuela', 'vegetarian', 'vegas', 'variety', 'vampiro', 'vacation', 'usnwsgov', 'urgent', 'unsafe', 'unknown', 'uniform', 'underway', 'undercover', 'unawares', 'unavoidable', 'ud', 'uber', 'tyre', 'ty', 'twins', 'turning', 'tunnel', 'tune', 'ts', 'trubgme', 'trs', 'triple', 'tries', 'trials', 'transporting', 'transformation', 'transfer', 'trafford', 'tr', 'towards', 'torch', 'tooth', 'tool', 'tonights', 'tomorrows', 'tmp', 'titan', 'tinyjecht', 'timeline', 'tilnow', 'til', 'tied', 'tie', 'ticket', 'thru', 'threats', 'third', 'thinks', 'thin', 'thick', 'the\\x89Û', 'theyd', 'therapy', 'that\\x89Ûªs', 'tflbusalerts', 'tf', 'tested', 'terrifying', 'terms', 'temper', 'tells', 'telegraph', 'teaching', 'teachers', 'tdp', 'tb', 'taxiways', 'targets', 'talks', 'talent', 'taco', 'tab\\x89Û', 'systems', 'sympathy', 'swing', 'swells', 'sweden', 'suv', 'survivor', 'surge', 'surface', 'surf', 'surely', 'surah', 'supervisor', 'superhero', 'suffering', 'suffered', 'stylish', 'stuck', 'strikes', 'stressed', 'stomach', 'stewart', 'stepped', 'stephen', 'step', 'steam', 'stays', 'startup', 'starring', 'standing', 'stamp', 'spinningbot', 'spinning', 'speed', 'specimens', 'specific', 'specif', 'spears', 'souls', 'songs', 'soldier', 'socialism', 'smoky', 'sm', 'slowly', 'slower', 'slip', 'slate', 'skirt', 'skills', 'sized', 'sittwe', 'sitting\\x89Û', 'sites', 'sink', 'silvergray', 'silent', 'significant', 'sight', 'showcase', 'shoulders', 'shook', 'shocked', 'shock', 'shitty', 'shirt', 'shipwreck', 'shelby', 'shedding', 'shared', 'shanghai', 'sexy', 'settlement', 'sends', 'sending', 'seemed', 'seasons', 'screenshots', 'scotland', 'scoopit', 'scifi', 'schiphol', 'scars', 'sb', 'savebees', 'sassy', 'sand', 'sampling', 'salvation', 'salmon', 'safely', 'saddlebrooke', 'rwy', 'rworldnews', 'runner', 'ruined', 'royal', 'rome', 'roller', 'role', 'robotrainstorm', 'robinson', 'roberts', 'roanoke', 'ripped', 'rid', 'rickperry', 'rexyy', 'revolution', 'reviews', 'revenues', 'returns', 'returned', 'retail', 'resulted', 'restaurant', 'responded', 'republic', 'repeat', 'reopening', 'reminder', 'relax', 'regret', 'registered', 'regarding', 'refused', 'reflect', 'referring', 'recycling', 'recover', 'recipes', 'recalls', 'realtime', 'realmandyrain', 'realdonaldtrump', 'raynbowaffair', 'range', 'ramag', 'rainfall', 'raid', 'radioactive', 'quotes', 'queens', 'purchase', 'pun', 'pull', 'ptsdchat', 'ptsd', 'psychological', 'psychiatric', 'provide', 'protests', 'propertycasualty', 'prompted', 'profit', 'proceeds', 'procedures', 'private', 'preview', 'present', 'premium', 'premature', 'prefer', 'prayers', 'posting', 'possibly', 'portion', 'porn', 'popular', 'pond', 'poland', 'pockets', 'plays', 'planning', 'pizza', 'pisgah', 'pillow', 'pieces', 'photographer', 'pets', 'period', 'peoples', 'penalties', 'peaceful', 'pattern', 'patched', 'patch', 'passed', 'panel', 'pam', 'palin', 'pack', 'outflow', 'os', 'ornament', 'origin', 'orange', 'oral', 'opus', 'options', 'ops', 'opposition', 'oper', 'openly', 'ooh', 'ontario', 'omars', 'olive', 'older', 'olap', 'ohio', 'offensiveåÊcontent', 'offensive\\x89Ûª', 'odeon', 'odds', 'odd', 'occurs', 'occupants', 'oc', 'oak', 'nyt', 'numbered', 'nuggets', 'nsfw', 'ns', 'nri', 'noticed', 'note', 'nose', 'noncompliant', 'non', 'noise', 'nm', 'nike', 'niece', 'nickcannon', 'nfl', 'newsintweets', 'neither', 'needle', 'necessary', 'nbcnews', 'navbl', 'nations', 'nasty', 'nap', 'musician', 'multiplayer', 'mullah', 'msf', 'movement', 'mountaineering', 'moth', 'moral', 'modiministry', 'modi', 'models', 'model', 'mnpdnashville', 'mkx', 'mitt\\x89Û\\x9d', 'mitigation', 'mistake', 'mirage', 'mins', 'minority', 'minor', 'minaj', 'milkshake', 'milk', 'militant', 'mike', 'mfs', 'mf', 'metrofmtalk', 'metrics', 'method', 'mercy', 'mercury', 'mentioned', 'mentally', 'mental', 'memphis', 'member', 'melt', 'mega', 'meat', 'mcilroy', 'mcdonalds', 'mattson', 'matters', 'mate', 'matches', 'marvel', 'marker', 'marine', 'marijuana', 'manmade', 'maj', 'magnum', 'ma', 'm194', 'luis', 'lrt', 'lover', 'loses', 'lonewolffur', 'log', 'locker', 'locke', 'lock', 'local\\x89Û', 'localarsonist', 'listenlive', 'listenbuy', 'limit', 'lighting', 'lgbt', 'lessons', 'lesbian', 'leo', 'len', 'leisure', 'lego', 'legion', 'legendary', 'leg', 'leadership', 'lay', 'launch', 'laugh', 'lately', 'las', 'languages', 'langley', 'landfall', 'laid', 'l', 'kuwait', 'kurtschlichter', 'kurdish', 'korea', 'komen', 'knoxville', 'kingdom', 'kiernan', 'kicked', 'key', 'kenya', 'karachi', 'kalle', 'just\\x89Û', 'jupiters', 'julie', 'journalist', 'jon', 'joke', 'joining', 'johnson', 'johannesburg', 'joel', 'jimmyfallon', 'jesus', 'jazz', 'jays', 'jay', 'jax', 'japans', 'jan', 'james', 'jail', 'j', 'i\\x89Ûªve', 'itunesmusic', 'irish', 'ireland', 'involved', 'integrity', 'instant', 'installation', 'inst', 'inning', 'ink', 'initial', 'inevitable', 'indonesia', 'individuals', 'individual', 'independent', 'imported', 'illinois', 'ik', 'idps', 'idiot', 'icymi', 'ibooklove', 'ianhellfire', 'ian', 'hybrid', 'hurry', 'hunk', 'hunger', 'hundred', 'humidity', 'humaza', 'httptcoo91f3cyy0r', 'httptconnmqlz91o9', 'httptcocybksxhf7d', 'httptcocedcugeuws', 'httptcobbdpnj8xsx', 'httptco9nwajli9cr', 'httpstcorqwuoy1fm4', 'httpstcodehmym5lpk', 'however', 'houston', 'horrors', 'honey', 'historic', 'hillary', 'hike', 'highly', 'higher', 'helps', 'headset', 'hating', 'hatchet', 'hatcap', 'hashtag', 'harrybecareful', 'harper', 'happiness', 'hang', 'hampshire', 'hamilton', 'hamas', 'haiyan', 'gunshot', 'gunsense', 'guilty', 'guided', 'guard', 'growth', 'grown', 'grove', 'groom', 'grew', 'greg', 'greeces', 'grand', 'grabbers', 'gps', 'gopdebate', 'gon', 'goals', 'glorious', 'gives', 'gerenciatodos', 'georgia', 'genuine', 'generally', 'gem', 'gateau', 'gamergate', 'gameplay', 'fyi', 'funds', 'fuels', 'fuckin', 'frozen', 'frontpage', 'freezing', 'frank', 'fraction', 'foul', 'formed', 'forgiven', 'ford', 'footage', 'foot', 'flying', 'flower', 'floated', 'flaming', 'fkn', 'fixed', 'fitness', 'fit', 'fishing', 'finishing', 'finish', 'finger', 'filled', 'ferguson', 'feminist', 'feelings', 'fed', 'features', 'fd', 'fb', 'favourite', 'fashionable', 'faroeislands', 'farm', 'fam', 'fallen', 'fair', 'facts', 'factors', 'extends', 'exploit', 'experiencing', 'experienced', 'expensive', 'expecting', 'existence', 'excellent', 'example', 'evidence', 'euro', 'eu', 'esp', 'er', 'eq', 'environmental', 'environment', 'entering', 'enrt', 'enjoyed', 'engine', 'ending', 'encounter', 'en', 'embroidered', 'email', 'elite', 'eight', 'ego', 'eggs', 'egged', 'ee', 'editor', 'ed', 'eb', 'eaten', 'eastern', 'earthquakes', 'ear', 'dundee', 'dudes', 'dublin', 'dual', 'drives', 'drinks', 'dreams', 'drag', 'dq', 'dorrets', 'dorret', 'donate', 'donald', 'domestic', 'doc', 'dj', 'disgusting', 'discovery', 'disasters', 'disappoints', 'disappearance', 'dis', 'dirt', 'diamondkesawn', 'devalue', 'detained', 'destroying', 'despite', 'designed', 'describing', 'describes', 'dept', 'dental', 'denmark', 'dem', 'deliver', 'delay', 'definitely', 'declined', 'december', 'decades', 'dating', 'dat', 'darkness', 'daesh', 'cutting', 'curb', 'cunts', 'cum', 'cue', 'cuban', 'cta', 'csx', 'cryptic', 'crosses', 'croatian', 'cries', 'cricket', 'crew', 'crashing', 'crackdown', 'counter', 'count', 'correction', 'corners', 'cooler', 'control\\x89Û\\x9d', 'continued', 'context', 'contemplating', 'contained', 'construction', 'consequence', 'conquest', 'connector', 'condition', 'concept', 'commute', 'communication', 'commit', 'comedy', 'combat', 'columbia', 'color', 'collision1141', 'college', 'cobra', 'coastal', 'coach', 'cnbc', 'clothes', 'closures', 'closing', 'closes', 'climb', 'clearedincident', 'claytonbryant', 'cladding', 'citizens', 'circuit', 'circle', 'cigarette', 'chunks', 'chosen', 'choking', 'choices', 'chocolate', 'chicken', 'chewing', 'chevy', 'chest', 'chernobyl', 'checked', 'chases', 'charlotte', 'charges', 'charger', 'chaos', 'changed', 'chain', 'centipede', 'cecilthelion', 'cd', 'cbcca', 'cave', 'catches', 'casual', 'casper', 'carried', 'cargo', 'capsized', 'capital', 'candy', 'camera', 'cam', 'calum5sos', 'cakes', 'caitlin', 'cabin', 'bypass', 'buying', 'butt', 'burnt', 'buried', 'bunch', 'bully', 'buddys', 'bu', 'btw', 'bts', 'bs', 'brush', 'briefing', 'brian', 'breathe', 'breaks', 'breakingnews', 'brave', 'brand', 'brady', 'bp', 'bowling', 'bounty', 'bottom', 'bother', 'born', 'bored', 'bore', 'border', 'boom', 'bookboost', 'bones', 'boko', 'boeing', 'bluedio', 'blowout', 'blowmandyup', 'blow', 'blasts', 'blanket', 'blame', 'blake', 'blah', 'birth', 'birds', 'bio', 'billings', 'bill', 'biker', 'bigamist', 'beware', 'betrayed', 'besides', 'bend', 'belongs', 'belonged', 'beginners', 'beforeitsnews', 'bee', 'becoming', 'beard', 'bbcnews', 'bb4sp', 'battleship', 'batters', 'bath', 'basement', 'bars', 'barn', 'barely', 'bare', 'barackobama', 'bangladesh', 'band', 'background', 'a\\x89Û', 'aware', 'aw', 'avenue', 'aussies', 'at\\x89Û', 'attitude', 'asshole', 'assembly', 'arwx', 'arts', 'artistsunited', 'arsenal', 'arrest', 'arm', 'appreciate', 'applications', 'apch', 'apartment', 'apart', 'aoms', 'anxious', 'anxiety', 'announcement', 'anna', 'angels', 'amirite', 'although', 'alternatives', 'alright', 'alex', 'alarms', 'ahh', 'affects', 'adding', 'acute', 'actual', 'acting', 'acid', 'accused', 'accionempresa', 'abortion', 'abomination', 'abe', 'abandon', '95', '87', '86', '80', '78', '6th', '69', '56', '55', '45', '43rd', '41', '375', '361', '300000', '2us', '27', '2005', '200', '1620', '150foot', '150401', '150', '103', '0day', '09', '010401', '0104', 'åÈ', 'åÇ', 'å', '\\x89Û÷plot', '\\x89Û÷it\\x89Ûªs', '\\x89ÛÏyou', '\\x89ÛÏstretcher', '\\x89ÛÏhannaph\\x89Û\\x9d', '\\x89Û¢åÊdemolition', 'zippednews', 'zionism', 'zimbabwe', 'zaynmalik', 'zabadani', 'yycstorm', 'ypres', 'you\\x89Ûªre', 'you\\x89Ûªll', 'yourselves', 'yougov', 'yesterdays', 'yellow', 'yelling', 'yazidi', 'yankees', 'xv', 'wwwcbplawyers', 'wwe', 'ww2', 'wud', 'wrist', 'wowo', 'wouldn\\x89Ûªt', 'worries', 'worn', 'worldwide', 'works', 'workplace', 'workout', 'worker', 'woods', 'woodlawn', 'wolves', 'wolf', 'wod', 'wocowae', 'wmv', 'wmur9', 'witness', 'wipp', 'wine', 'windy', 'wilshere', 'william', 'wide', 'whose', 'whoops', 'whomever', 'wholesale', 'whoever', 'whoa', 'whitehouse', 'whitbourne', 'whistle', 'whipped', 'whenever', 'whatsapp', 'whale', 'wftv', 'wfp', 'weyreygidi', 'weston', 'wen', 'welfare', 'weighs', 'weekold', 'wednesdays', 'wedn', 'wedaug5th', 'wealthy', 'weallheartonedirection', 'weakening', 'wd', 'wce', 'wbioterrorismampuse', 'wb', 'wayne', 'wattpad', 'watertown', 'waters', 'washingtonpost', 'warrior', 'warming', 'warm', 'walter', 'walked', 'walerga', 'waking', 'waits', 'waist', 'voting', 'voted', 'voodoo', 'voluntary', 'volleyball', 'volcanoåÊinåÊrussia', 'volcanoes', 'vods', 'vital', 'visto', 'visits', 'vision', 'violations', 'vince', 'vietnamese', 'videoveranomtv', 'vida', 'victory', 'vicinity', 'vice', 'vibez', 'vi', 'veteran', 'versions', 'versethe', 'venice', 'vegetable', 've', 'vault', 'vashon', 'various', 'vanuatu', 'vantage', 'vancouver', 'utc5km', 'ut', 'ushed', 'user', 'us101', 'urs', 'upwards', 'uploading', 'upgrading', 'upgraded', 'unrest', 'unrelenting', 'unr', 'unprecedented', 'unnecessary', 'unity', 'unique', 'unions', 'uniforms', 'unhappiness', 'unfortunately', 'unfolded', 'uncle', 'ultimalucha', 'ukraine', 'typos', 'types', 'tyler', 'twovehicle', 'twos', 'twentynine', 'tweetlikeitsseptember11th2001', 'turkmen', 'tuning', 'tunein', 'tuesday', 'ttes', 'tt', 'tsunamiesh', 'tryouts', 'tryout', 'trusty', 'trunk', 'truelove', 'troy', 'troubling', 'troll', 'trinity', 'trillion', 'trick', 'trend', 'tremor', 'trek', 'treeporn', 'treatments', 'treating', 'treated', 'travis', 'traverse', 'travelling', 'traveling', 'transformed', 'trancy', 'tragic', 'trading', 'touched', 'totes', 'torture', 'tornados', 'torching', 'tor', 'topstories', 'tops', 'tomcatarts', 'tomatoes', 'tokyo', 'tlc', 'tix', 'titanic', 'tin', 'timkaine', 'timelapse', 'tigers', 'tides', 'tidal', 'thyroid', 'thy', 'thuggin', 'thu', 'thrown', 'throwingknifes', 'throwback', 'throughout', 'thrones', 'throat', 'threw', 'threaten', 'thisizbwright', 'thisiswhywecanthavenicethings', 'thirsty', 'thirst', 'thighs', 'thief', 'thetawniest', 'therapies', 'theological', 'themed', 'themagickidraps', 'thegame', 'thee', 'thankfully', 'thankful', 'tha', 'tgirl', 'tests', 'testing', 'testimony', 'teslas', 'term', 'tens', 'temptation', 'teeth', 'teens', 'teenagers', 'technologies', 'technique', 'technica', 'teases', 'tear', 'teamstream', 'teamhendrick', 'teacher', 'te', 'tch', 'tc', 'taylor', 'taught', 'tattoo', 'tastes', 'tasmanias', 'targeted', 'tape', 'tanzania', 'tanks', 'tan', 'talkin', 'tale', 'tak', 'tahoe', 's\\x89Û', 'syndrome', 'symphony', 'sx', 'sws', 'swept', 'sweet2young', 'sweater', 'sweat', 'swear', 'swarm', 'sustainable', 'suspense', 'suryaray', 'survey', 'suruc', 'surrounding', 'surrender', 'surgery', 'surfers', 'suppose', 'supports', 'supply', 'sunny', 'sunnis', 'sundays', 'summit', 'summertime', 'suit', 'suicidebombing', 'suh', 'suddenly', 'sudden', 'subway', 'stuns', 'stunned', 'studio', 'student', 'stu', 'sts', 'struggling', 'strongly', 'stronger', 'stroke', 'striking', 'strikesstrikes', 'strikers', 'striker', 'strict', 'stressful', 'strengthening', 'stout', 'stormchase', 'stores', 'stolen', 'stole', 'stockton', 'stocks', 'sticks', 'stick', 'steal', 'steak', 'stats', 'state\\x89Û', 'startups', 'starbucks', 'stans', 'standwithpp', 'standards', 'stagetwo', 'stacey', 'sr', 'squeeze', 'square', 'sputnik', 'spreading', 'spots', 'sportwatch', 'spontaneously', 'split', 'spilled', 'spending', 'spencers', 'spectacular', 'species', 'specialists', 'specialist', 'speak', 'soviet', 'southdowns', 'soudelors', 'sort', 'sorrows', 'sore', 'soooo', 'sooo', 'soo', 'sons', 'somewhere', 'solve', 'solo', 'solitude', 'solid', 'solicitor', 'sold', 'software', 'softenza', 'soft', 'socialmedia', 'soccer', 'soaking', 'soak', 'snuff', 'snowden', 'snowball', 'sniping', 'sneak', 'snd', 'snapchat', 'snacks', 'smooth', 'smiling', 'smells', 'smell', 'smash', 'slips', 'slipped', 'slides', 'slicker', 'slayer', 'slammed', 'skys', 'skyline', 'skipping', 'skill', 'sketch', 'skanndtyagi', 'sixpenceee', 'sixmeter', 'sis', 'singles', 'sin', 'simultaneous', 'simply', 'simon', 'signup', 'signing', 'sigalert', 'siding', 'sides', 'si', 'shutdown', 'showing', 'showed', 'shouting', 'shout', 'shouldve', 'shore', 'shooter', 'shoot', 'shoe', 'shizune', 'shira', 'sheriff', 'shelli', 'sheet', 'sheeran', 'sheer', 'shed', 'shaw', 'sharing', 'sharethis', 'shantae', 'shania', 'sham', 'shaking', 'shakes', 'shaker', 'sf', 'sewer', 'settle', 'setlist', 'servers', 'server', 'serve', 'servants', 'sequel', 'sept', 'sep', 'selling', 'sell', 'selfies', 'selection', 'seem', 'seduction', 'seal', 'screw', 'screamqueens', 'scream', 'scottwalker', 'scorpions', 'schwarber', 'schedule\\x89Û', 'scares', 'scare', 'scale', 'saturn', 'satan', 'sarcasm', 'sandy', 'sanctioned', 'samesex', 'samanthaturne19', 'salvis', 'salvador', 'sake', 'sailing', 'sail', 'sahib', 'sacramento', 'sa15t', 's2g', 'ryan', 'rutherford', 'rush', 'ruins', 'ruining', 'ruebs', 'rudd', 'rtrrt', 'rtamerica', 'rss', 'rpics', 'royals', 'royalcarribean', 'rover', 'routecomplex', 'rotten', 'rotator', 'rossum', 'roses', 'rory', 'ronnie', 'ronaldo', 'rolls', 'rohnertparkdps', 'roh3', 'rogue', 'roger', 'rockbottomradfm', 'riyadh', 'riversiskiyou', 'risks', 'rio', 'rihanna', 'rightwaystan', 'rightways', 'rifles', 'riding', 'riders', 'rick', 'rich', 'rey', 'revelation', 'rev', 'retweet', 'retro', 'resulting', 'restricted', 'restive', 'restaurants', 'responder', 'respected', 'resources', 'reshapes', 'required', 'request', 'repay', 'repairs', 'rep', 'reopens', 'reopen', 'renison', 'removing', 'removed', 'removal', 'remorse', 'remix', 'religion', 'relentless', 'relationship', 'rejects', 'rejected', 'reject', 'reinstate', 'reids', 'reid', 'regional', 'regardless', 'regard', 'refuse', 'redeemeth', 'recreates', 'recording', 'recommended', 'receive', 'recap', 'realised', 'reads', 'reacted', 'react', 'reaches', 'rdhorndale', 'rcmp', 'rb', 'raÌ¼l', 'razing', 'raynor', 'rayner', 'raw', 'raung', 'rating', 'rates', 'rated', 'rat', 'rapping', 'rappers', 'rapper', 'rapidcity', 'rap', 'random', 'rammed', 'raised', 'rainier', 'rained', 'railways', 'railway', 'rails', 'raiders', 'rages', 'racing', 'quoted', 'questioning', 'que', 'quartz', 'quarters', 'q', 'p\\x89Û', 'putins', 'pushing', 'push', 'purchased', 'puppy', 'puppies', 'punjab', 'punishment', 'punished', 'puncture', 'pulwama', 'pulling', 'publicizing', 'ptbo', 'pt', 'psalms', 'ps4', 'proud', 'protesting', 'protection', 'protecting', 'protectdenaliwolves', 'prosyn', 'proposed', 'proper', 'proof', 'prone', 'promote', 'projects', 'programme', 'professional', 'product', 'prod', 'privacy', 'priority', 'priorities', 'primarily', 'priceless', 'previous', 'prevention', 'pretend', 'pressed', 'preservation', 'presence', 'prepper', 'preparing', 'preorder', 'premonitions', 'premiere', 'premier', 'prediction', 'predator', 'precious', 'pre', 'potentially', 'posters', 'postbattle', 'possibility', 'poss', 'portugal', 'popularmmos', 'pops', 'poplar', 'pope', 'poorly', 'pomo', 'politifiact', 'policerun', 'pointing', 'poem', 'pocket', 'po', 'pmharper', 'pmarca', 'pluto', 'plummeted', 'pllolz', 'plenty', 'pledge', 'pleasure', 'platform', 'plants', 'planes', 'placing', 'placed', 'pjnet', 'pits', 'pitchers', 'pissed', 'pinpoint', 'piner', 'pillows', 'pill', 'pictures', 'pickup', 'pickerel', 'physicians', 'photoset', 'philadelphia', 'phil', 'phew', 'phase', 'pharaoh', 'phantom', 'peterjukes', 'pertains', 'personally', 'perquisite', 'permits', 'permanently', 'permanent', 'performing', 'performance', 'per', 'pennington', 'penn', 'pendleton', 'peeps', 'pedestrians', 'pedestrian', 'peak', 'peacefully', 'pdx911', 'pcps', 'pays', 'paved', 'pattonoswalt', 'patriot', 'pathogen', 'pasta', 'password', 'partnerships', 'particularly', 'paris', 'paramedic', 'parade', 'pants', 'pan', 'pakpattan', 'pakistannews', 'pair', 'paint', 'paid', 'pa', 'ovofest', 'ovo', 'overwork', 'overlooking', 'overload', 'ouvindo', 'outfit', 'outbid', 'outage', 'ourselves', 'otherwise', 'oth', 'oso', 'osborn', 'originalfunko', 'organizations', 'organic', 'org', 'oregon', 'ordered', 'opp', 'operations', 'opera', 'oped', 'oooooohhhh', 'oooh', 'oo', 'onlinecommunities', 'oneself', 'omega', 'oks', 'okinawa', 'ohh', 'offs', 'offramp', 'offr', 'offices', 'offers2go', 'obviously', 'obsessed', 'objects', 'obispo', 'nylon', 'nwo', 'nv', 'nurses', 'nuff', 'nude', 'ntsb', 'nowhere', 'november', 'novel', 'notifications', 'noted', 'nosurrender', 'nostrils', 'nostalgia', 'northwest', 'northland', 'northeast', 'norman', 'nor', 'nope', 'nonlife', 'noaa', 'njturnpike', 'njenga', 'nj', 'nixon', 'nikeplus', 'nights', 'nightmare', 'nickcocofree', 'nick', 'niall', 'nhs', 'nh', 'new\\x89Û', 'newzsacramento', 'newswatch', 'newsarama', 'newbie', 'newberg', 'neverending', 'netanyahu', 'nestleindia', 'nepal', 'neighborhood', 'neck', 'nda', 'nbc', 'nba', 'nazi', 'native', 'nasasolarsystem', 'narendramodi', 'nankana', 'nan', 'names', 'najib', 'nail', 'm\\x89Û', 'myfitnesspal', 'mv', 'muzzamil', 'mutual', 'mutant', 'musik', 'musicians', 'murders', 'murderers', 'mum', 'multidimensi', 'multi', 'mukilteo', 'mud', 'msnbc', 'moyo', 'mouth', 'mourns', 'moulding', 'motion', 'moscow', 'mooresville', 'montgomery', 'montetjwitter11', 'monster', 'monkeys', 'monkey', 'monitoring', 'monitor', 'moms', 'mohammed', 'moderate', 'mock', 'mobile', 'mmmmmm', 'ml', 'miyagi', 'mistreated', 'mississauga', 'missionhills', 'missiles', 'misses', 'mishacollins', 'misery', 'ministers', 'minions', 'minimehh', 'minhazmerchant', 'milwaukee', 'migrants\\x89Û', 'midwest', 'midsouth', 'midget', 'mid', 'microsofts', 'microsoft', 'microphone', 'micom', 'michelebachman', 'mic', 'mi17', 'mgm', 'metlife', 'meteoearth', 'met', 'messi', 'messed', 'messages', 'mercados', 'meme', 'meinlcymbals', 'megynkelly', 'meets', 'medium', 'medieval', 'medicine', 'medals', 'mechanical', 'measles', 'meaning', 'mcmahon', 'mchenry', 'mc', 'matthew', 'material', 'matching', 'master', 'massmurderer', 'martinmj22', 'marshall', 'married', 'marquei', 'marlon', 'marketing', 'marines', 'mariah', 'march', 'maps', 'manufactured', 'mania', 'mandatory', 'mamata', 'malik', 'maketh', 'maker', 'maintain', 'magginoodle', 'mafia', 'madison', 'lyrics', 'lying', 'luka', 'luchaunderground', 'lubbock', 'lovedup', 'lous', 'lotz', 'loseit', 'losdelsonido', 'loretta', 'looses', 'looping', 'longterm', 'longs', 'lone', 'londonfire', 'lolol', 'lollapalooza', 'logistics', 'logic', 'locked', 'located', 'loans', 'loads', 'loading', 'loaded', 'lmfaoooo', 'llf', 'lizards', 'livingsafely', 'liveonk2', 'lived', 'lith', 'lit', 'links', 'lines', 'likes', 'lightningcaused', 'lifting', 'lifetime', 'lifethreatening', 'lifestyle', 'liberal', 'liable', 'liability', 'lglorg', 'letter', 'letsfootball', 'lesson', 'lemon', 'legs', 'legislation', 'legio', 'leeds', 'learned', 'leaks', 'leaked', 'leak', 'layout', 'lawsuit', 'lawrence', 'lavenderpoetrycafe', 'lauren', 'laundry', 'latimes', 'latestnews', 'lasting', 'laois', 'lansdowne', 'language', 'lanes', 'landscape', 'lands', 'lance', 'lamha', 'lakes', 'labs', 'label', 'kurd', 'ks94', 'ks', 'kraft', 'kowing', 'koin6news', 'knowing', 'knob', 'knife', 'knees', 'km', 'kittens', 'kith', 'kisses', 'kissed', 'kiss', 'kings', 'kindness', 'kindermorgan', 'killings', 'killers', 'kiev', 'kidding', 'kerry', 'keratin', 'kendall', 'ke', 'katunews', 'katrina', 'katherines', 'kashmir', 'karma', 'kanye', 'kaduna', 'justmarried', 'justifying', 'justified', 'jurors', 'junk', 'june', 'jumped', 'juliedicaro', 'juice', 'jr', 'jp', 'journal', 'jose', 'jonvoyage', 'jones94kyle', 'jones', 'jonathanferrell', 'joint', 'johns', 'johnny', 'jkl', 'jim', 'jhaustin', 'jewelry', 'jerry', 'jenner', 'jeff', 'jealous', 'jdabe80', 'japton', 'jams', 'jamesmelville', 'jamaicaplain', 'jamaicaobserver', 'jailed', 'jacque', 'jacket', 'ja', 'i\\x89Ûªd', 'ivanberroa', 'itsjustinstuart', 'itll', 'items', 'item', 'isil', 'irony', 'ironic', 'iredell', 'iranians', 'ipod', 'ipad', 'in\\x89Û', 'investing', 'investigation', 'investigate', 'invest', 'invasion', 'interviews', 'interstate', 'interlaken', 'intelligence', 'intact', 'insubcontinent', 'inspiring', 'inspections', 'insomnia', 'insight', 'insas', 'innovation', 'injures', 'infosec', 'infinity', 'inferno', 'infectious', 'inevitably', 'inec', 'indoors', 'indie', 'indiannews', 'indi', 'incredibly', 'increase', 'incase', 'inc', 'impulse', 'improve', 'impressive', 'impressed', 'important', 'imperfect', 'impending', 'impacted', 'immediately', 'imdb', 'imagined', 'ima', 'illustration', 'ihhen', 'ignored', 'ignore', 'idol', 'idis', 'identitytheft', 'idc', 'iconic', 'icelandreview', 'i95', 'i580', 'i405', 'i10', 'h\\x89Û', 'hysteria', 'hypocrisy', 'hyderabad', 'hurricanes', 'hurricanedolce', 'humble', 'humanconsumption', 'hug', 'httpt\\x89Û', 'httptcozevakjapcz', 'httptcozdtoyd8ebj', 'httptcoyduixefipe', 'httptcoydetwgribk', 'httptcoxssgedsbh4', 'httptcoxpfmr368uf', 'httptcovz1irh0nmm', 'httptcovxvfaeey0q', 'httptcovam5podgyw', 'httptcov3azwoamzk', 'httptcouoozxaus26', 'httptcoskqpwsnoin', 'httptcosdgoutwntb', 'httptcosaf9mosksn', 'httptcoroi2nsmejj', 'httptcopyehwodwun', 'httptcopvmr38lnva', 'httptcopo19h8ycnd', 'httptcophixznv1yn', 'httptconmfsgkf1za', 'httptcomg5eajelul', 'httptcomfckpvzfv8', 'httptcom5kxlpkfa1', 'httptcom203ul6o7p', 'httptcolxtjc87kls', 'httptcolwwojxttiv', 'httptcojhpdssvhve', 'httptcoj5mkcbkcov', 'httptcoio7kuug1uq', 'httptcoiikssjgbdn', 'httptcoidmhswewqw', 'httptcoi27oa0hisp', 'httptcoeysvvza7qm', 'httptcoedyfo6e2pu', 'httptcodydfvz7amj', 'httptcoct9ejxolpu', 'httptcoc1h7jecfrv', 'httptcobtdjgwekqx', 'httptcoafmkcfn1tl', 'httptco7hanpcr5rk', 'httptco3tj8zjin21', 'httptco3sicroaanz', 'httptco0wratka2jl', 'httpstcowudlkq7ncx', 'httpstcolfkmtzaekk', 'httpstcoe8dl1lncvu', 'hr', 'housed', 'hottest', 'hostageamp2', 'hospitals', 'hoping', 'hood', 'honest', 'homs', 'homie', 'homeowners', 'holmgren', 'holed', 'holds', 'hoes', 'hoe', 'hockey', 'hmu', 'hmm', 'hiroshima70', 'hiphop', 'hinton', 'himalaya', 'hiding', 'hide', 'hidden', 'hew', 'heros', 'heroin', 'hermancranston', 'henry', 'helpline', 'hella', 'helicopters', 'heavenly', 'heartwarming', 'heartless', 'hearthstone', 'healthy', 'healthcare', 'healing', 'heal', 'headlines', 'hazmat', 'have\\x89Û', 'hatred', 'hatchetwielding', 'harwich', 'harmkid', 'hardy', 'hardline', 'hardcore', 'happily', 'handling', 'handle', 'hamburg', 'halt', 'hall', 'hahahaha', 'hahahah', 'hahaha', 'hahah', 'hah', 'hadnt', 'hackers', 'hack', 'habits', 'gym', 'gust', 'gunmen', 'gum', 'guest', 'guaranteed', 'gtii', 'growingupblack', 'growing', 'grounds', 'grenades', 'greenway', 'greenharvard', 'greek', 'greatbritishbakeoff', 'graveyard', 'gravel', 'grass', 'grandpa', 'grandeur', 'grande', 'gpm', 'governments', 'goulburn', 'gorgeous', 'goodbye', 'golem', 'goldstein', 'goku', 'gofundme', 'goat', 'gn', 'gmt', 'gmmbc', 'gloucester', 'globe', 'globalwarming', 'glink', 'glimpses', 'glenn', 'glasses', 'giveaway', 'gilbert23', 'gig', 'gif', 'ghostwriter', 'gg', 'geometric', 'genius', 'geneva', 'generalnews', 'gel', 'gear', 'gd', 'gays', 'gawx', 'gates', 'garfield', 'gardens', 'garbage', 'gander', 'gaining', 'gain', 'gadgets', 'ga', 'fury', 'function', 'fukushimatepco', 'fueling', 'fruits', 'frontlines', 'friggin', 'friendship', 'fresno', 'freshman', 'freaking', 'franklin', 'fran', 'fragile', 'foxnew\\x89Û', 'fouseytube', 'fourth', 'foster', 'for\\x89Û', 'fortune', 'fortunately', 'forth', 'forsure', 'forgive', 'forex', 'foreign', 'forbid', 'fool', 'foodscare', 'follower', 'folk', 'fold', 'fm', 'fluid', 'flow', 'florida\\x89Û', 'float', 'flip', 'fleeing', 'fled', 'flashbacks', 'fixing', 'fist', 'fire\\x89Û', 'firey', 'firetruck', 'firemen', 'firefighting', 'fir', 'finished', 'financing', 'fill', 'files', 'file', 'fifty', 'fifa16', 'fierce', 'fieg', 'fi', 'fevwarrior', 'fest', 'ferries', 'ferguson\\x89Ûªs', 'fergusons', 'fennovoima', 'fella', 'feinstein', 'federal', 'featured', 'feast', 'favs', 'favorites', 'favor', 'faulty', 'fathers', 'fatburning', 'fatally', 'fatalityus', 'fart', 'familia', 'fallacy', 'fairy', 'fairfax', 'fading', 'facing', 'facility', 'fabric', 'extinguished', 'exterminate', 'extensive', 'extension', 'extender', 'explores', 'explodes', 'explain', 'expand', 'exited', 'exhibition', 'exhausted', 'exercised', 'executing', 'excuse', 'exclusive', 'exciting', 'excited', 'exc', 'examiner', 'evolve', 'everytime', 'everybody', 'evansville', 'evanston', 'evacuating', 'ev', 'eurotunnel', 'eudrylantiqua', 'etisalat', 'et', 'esteemed', 'esh', 'escaping', 'escaped', 'ergo', 'equate', 'epicenter', 'en\\x89Û', 'envw98', 'entrepreneur', 'enroute', 'enormous', 'enhanced', 'engvaus', 'engage', 'enemies', 'endures', 'endorses', 'endangered', 'encouragement', 'encore', 'enabled', 'empty', 'empire', 'emperor', 'emotionally', 'emerges', 'emerg', 'elsa', 'elliott', 'elkhorn', 'elizabeth', 'element', 'electricity', 'eh', 'egypt', 'efak', 'ef5', 'edge', 'economies', 'eastward', 'ears', 'earnings', 'earned', 'earbuds', 'dystopian', 'dye', 'dwarves', 'dvc', 'durant', 'dunbar', 'dumb', 'dukes', 'ducks', 'duck', 'dt', 'ds', 'dryer', 'drum', 'dropping', 'droid', 'drivers', 'drifting', 'drew', 'dressed', 'dreaming', 'draw', 'drama', 'dozen', 'downstairs', 'downpours', 'downfall', 'douchebag', 'doublecups', 'dothraki', 'dorman', 'dopey', 'dope', 'doors', 'donå«t', 'dolphin', 'dollars', 'document', 'doctors', 'doctor', 'dock', 'dmpl', 'dm', 'dlh', 'divided', 'district', 'distinct', 'disruptive', 'displace', 'discussion', 'disappeared', 'disappear', 'directors', 'director', 'diplomacy', 'dijk\\x89Ûª', 'dignity', 'digits', 'digital', 'difficult', 'diesis', 'dickheads', 'dick', 'diarrhea', 'diaporama', 'diamorfiend', 'diablo', 'dey', 'devil', 'device', 'develop', 'detroit', 'destructive', 'destroyer', 'despair', 'desk', 'deserves', 'deserve', 'descriptions', 'derby', 'deputy', 'deny', 'denier', 'denial', 'denali', 'dems', 'demon', 'democrats', 'demi', 'delete', 'delayed', 'del', 'define', 'defects', 'def', 'deeds', 'decor', 'declared', 'deck\\x89Û\\x9d', 'deck', 'decent', 'debatequestionswewanttohear', 'dealbreaker', 'deadliest', 'daytoday', 'davidcameron', 'dates', 'darkest', 'dante', 'dannyonpc', 'danisnotonfire', 'daniels', 'dances', 'dambisa', 'damaging', 'cyclists', 'cycling', 'customs', 'customers', 'custom', 'custer', 'cus', 'curiosity', 'cuff', 'ctd', 'cs', 'crusty', 'crude', 'crowns', 'croat', 'critical', 'crippling', 'criminal', 'crimes', 'cried', 'crickets', 'credit', 'creating', 'creates', 'crane', 'cracks', 'cracking', 'crack', 'coyotes', 'coworker', 'cowboys', 'cow', 'coverage', 'cousins', 'courts', 'coursing', 'countynews', 'countrys', 'countless', 'couldve', 'costly', 'correspondent', 'correct', 'corp', 'corleonedaboss', 'corey', 'coral', 'copycats', 'coping', 'cope', 'cooking', 'conversations', 'conversation', 'contruction', 'controllers', 'controlled', 'contributing', 'contrast', 'contract', 'continually', 'contain', 'contact', 'constant', 'console', 'conservative', 'consent', 'connectorconnecto', 'condolence', 'condemnation', 'condemn', 'concrete', 'concluded', 'con', 'compound', 'complications', 'completed', 'complaints', 'compete', 'comparison', 'comp', 'como', 'committee', 'committed', 'commerce', 'command', 'colluded', 'collective', 'cole', 'colder', 'col', 'coincide', 'cod', 'cockpit', 'cocaine', 'coat', 'coaster', 'cnewslive', 'cmon', 'cm', 'clueless', 'closest', 'cliffs', 'clever', 'cleric', 'cleared', 'cld', 'claimed', 'cjoyner', 'civilization', 'cityofcalgary', 'cites', 'circus', 'chronicle', 'christie', 'christ', 'chris', 'choose', 'chills', 'chiefs', 'chick', 'cheyenne', 'chevrolet', 'chesttorso', 'cheryl', 'cherry', 'cherokee', 'chase', 'channels', 'changing', 'chances', 'chan', 'championship', 'challenges', 'chairs', 'chairman', 'chair', 'cfc', 'certified', 'certificate', 'centers', 'census', 'cement', 'celebrations', 'cbsbigbrother', 'cbs', 'cbc', 'cawx', 'catching', 'castle', 'casperrmg', 'cash', 'cart', 'carr', 'carpet', 'carlos', 'caribbean', 'careers', 'cape', 'canvas', 'cannon', 'candle', 'cancers', 'cancels', 'cancelled', 'canadas', 'camps', 'camping', 'cameo', 'calumet', 'cairo', 'cafire', 'cafe', 'cadfyi', 'cables', 'cab', 'c4news', 'b\\x89Û', 'bwp', 'butterfinger', 'busy', 'bundle', 'bumper', 'bump', 'bullseye', 'bullets', 'builds', 'buffer', 'bubble', 'brunette', 'bruise', 'brooke', 'bronx', 'britons', 'britney', 'brisk', 'brighton', 'brigade', 'brief', 'brick', 'brewing', 'breathing', 'bread', 'brazilian', 'bradleybrad47', 'boxing', 'bounds', 'boundary', 'boundaries', 'bottle', 'boise', 'bodys', 'bodybagging', 'bobcats', 'bmw', 'blutz10', 'blunt', 'blues', 'blueprint', 'bloomberg', 'blocks', 'blizzheroes', 'blizzarddraco', 'blames', 'blackpool', 'blacklivesmatter', 'bits', 'bites', 'bite', 'bitcoin', 'bistro', 'birmingham', 'biological', 'bills', 'billneelynbc', 'billboard', 'bicycles', 'bicycle', 'bicep', 'betz', 'bets', 'bethlehem', 'bestseller', 'bengal', 'belt', 'belly', 'believing', 'beliefs', 'belief', 'behalf', 'begging', 'beclearoncancer', 'beckarnley', 'beauty', 'bean', 'battles', 'batting', 'batteries', 'battered', 'basis', 'basically', 'bashes', 'baruch', 'barrier', 'banquet', 'bangin', 'banerjee', 'bancodeseries', 'baltimore', 'balls', 'balance', 'bakeofffriends', 'bail', 'bago', 'backyards', 'backyard', 'backup', 'backs', 'ay', 'awwww', 'awareness', 'awaits', 'avoided', 'aviation', 'avengers', 'available', 'autumn', 'autoinsurance', 'autistic', 'author', 'authentic', 'australia\\x89Ûªs', 'austin', 'aust', 'aunt', 'audience', 'auction', 'auckland', 'attraction', 'attic', 'attendance', 'attempting', 'attempt', 'atmospheric', 'atmosphere', 'atm', 'atlantic', 'atk', 'athlete', 'astrology', 'association', 'assisting', 'assistant', 'assholes', 'asleep', 'asks', 'askcharley', 'asics', 'ashley', 'ashayo', 'ash', 'asf', 'artist', 'artificial', 'arsonistmusic', 'ars', 'arriving', 'arnhem', 'armory', 'armed', 'arizona', 'ariaahrary', 'argument', 'area\\x89Û', 'architecture', 'architect', 'aquarium', 'apt', 'april', 'approves', 'approval', 'appropriation', 'approaching', 'appreciated', 'appointment', 'applies', 'applaud', 'apollobrown', 'apocalyptic', 'apartments', 'aogashima', 'anyways', 'any1', 'antonio', 'annoying', 'announces', 'announced', 'ani', 'anger', 'andrew', 'andor', 'ames', 'ambulances', 'alot', 'alois', 'ally', 'alloy', 'allies', 'allegiance', 'allegations', 'aliens', 'alien', 'alexbelloli', 'alerts', 'albany', 'alarmed', 'ak', 'agrees', 'aggressive', 'ages', 'agents', 'agency', 'ag', 'africans', 'afp', 'affiliation', 'affiliate', 'affecting', 'afc', 'advised', 'advice', 'advertised', 'adventures', 'advanced', 'admit', 'administration', 'admin', 'addition', 'adam', 'ad', 'actor', 'activities', 'acted', 'acquire', 'acoustic', 'achimota', 'accustomed', 'accounts', 'accept', 'acc', 'ac', 'abusing', 'abuseddesolateamplost', 'abused', 'abubaraa1', 'absolute', 'abomb', 'abia', 'abc7', 'abbott', 'aa', 'a5', 'a1', '9pm', '9newsgoldcoast', '9am', '96', '90th', '90blksamp8whts', '9000', '900', '8th', '8pin', '84', '83', '77', '75', '72w', '64', '61st', '60mph', '60000', '5th', '5sosfam', '5sos', '5s', '5c', '57', '548', '530', '53', '4th', '4playthursdays', '4km', '4500feet', '43', '429cj', '3m', '3inspired', '3942', '360', '36', '32', '2k15', '299', '29', '28', '23km', '235409', '233liveonline', '21a', '211023', '2082676773', '2016', '20150805', '2012', '2010', '2008', '2000', '1999', '1998', '1986', '1979', '1976', '1974', '1970', '1965', '1943', '1916', '18wheeler', '1880', '180', '175225', '1716', '166', '15th', '15km', '143', '140', '12news', '118', '1100', '10km', '109', '1061thetwister', '1038pm', '1030', '100000', '075', '0700', '070', '06jst', '0306', '015025', '005225', 'åÊi', 'åÊfedex', 'å¬only', 'å©daniel', 'å¤', 'å£9', 'å£6bn', 'å£27900end', 'å£150', 'å£100bn', 'Ìü', 'ÌÑ1', 'ÌÑ', 'Ì¢', '\\x89âÂ', '\\x89Û÷we', '\\x89Û÷vulnerable\\x89Ûª', '\\x89Û÷the', '\\x89Û÷second\\x89Ûª', '\\x89Û÷ransomware\\x89Ûª', '\\x89Û÷nuclear', '\\x89Û÷nother\\x89Û\\x9d', '\\x89Û÷muslim', '\\x89Û÷minimum', '\\x89Û÷let\\x89Ûªs', '\\x89Û÷leaves', '\\x89Û÷it', '\\x89Û÷institute', '\\x89Û÷ill', '\\x89Û÷hoax', '\\x89Û÷hijacker', '\\x89Û÷heat', '\\x89Û÷hazard\\x89Ûª', '\\x89Û÷good', '\\x89Û÷food', '\\x89Û÷first\\x89Ûª', '\\x89Û÷faceless\\x89Ûª', '\\x89Û÷faceless', '\\x89Û÷exceptional\\x89Ûª', '\\x89Û÷em', '\\x89Û÷devastated\\x89Ûª', '\\x89Û÷british', '\\x89Û÷bomb', '\\x89Û÷body', '\\x89Û÷badges', '\\x89Û÷avalanche\\x89Ûª', '\\x89Û÷amino', '\\x89Û÷alloosh', '\\x89ÛÓkody', '\\x89ÛÓher', '\\x89ÛÒåÊcnbc', '\\x89ÛÒthe', '\\x89ÛÏymcglaun', '\\x89ÛÏthehighfessions', '\\x89ÛÏthat\\x89Ûªs', '\\x89ÛÏsippin\\x89Ûª', '\\x89ÛÏplans', '\\x89ÛÏparties', '\\x89ÛÏnumbers', '\\x89ÛÏnobody', '\\x89ÛÏmake', '\\x89ÛÏmacdaddyleo', '\\x89ÛÏlordbrathwaite', '\\x89ÛÏlolgop', '\\x89ÛÏleoblakecarter', '\\x89ÛÏleejasper', '\\x89ÛÏkeits', '\\x89ÛÏi', '\\x89ÛÏfor', '\\x89ÛÏfdny', '\\x89ÛÏdylanmcclure55', '\\x89ÛÏdetonate\\x89Û\\x9d', '\\x89ÛÏcat', '\\x89ÛÏbbcwomanshour', '\\x89ÛÏbbcengland', '\\x89ÛÏbasedgeorgie', '\\x89ÛÏall', '\\x89ÛÏairplane\\x89Û\\x9d', '\\x89ÛÏ', '\\x89Û¢\\x89Û¢if', '\\x89Û¢im', '\\x89Û¢', 'zzzz', 'zxathetis', 'zurich', 'zumiez', 'zss', 'zrnf', 'zourryart', 'zotar50', 'zoom', 'zonewolf123', 'zonesthank', 'zones', 'zombies', 'zombiefunrun2014', 'zomatoaus', 'zojadelin', 'zodiac', 'zmne', 'ziuw', 'zippoline', 'zippers', 'zipper', 'zipped', 'ziphimup', 'zip', 'zionists', 'zimmerman', 'zimmer', 'zicac', 'zhenghxn', 'zhejiang', 'zeros', 'zero', 'zergele', 'zeno001', 'zenandemcfen', 'zehrs', 'zeal', 'zaynmaiikist', 'zarry', 'zarharzar', 'zar', 'zamtriossu', 'zaman', 'zakuun', 'zakbagans', 'zaibatsunews', 'zachzaidman', 'zach', 'zacb', 'zaatari', 'z3kesk1', 'yzf', 'yyj', 'yyeso', 'yycweather', 'yycfringe', 'yuvi', 'yuuko', 'yuppies', 'yup', 'yunita99', 'yumiko', 'yum', 'yukis', 'yug', 'yuan', 'ypg', 'yo\\x89Û', 'you\\x89Û\\x9d', 'you\\x89Û', 'youuu', 'youssefyamani', 'yourboyshawn', 'younoone', 'young\\x89Û', 'youngsafe', 'youngins', 'youngerampgrossly', 'younger', 'yosemite', 'yorkshire', 'yor', 'yonews', 'yolk', 'yolandaph', 'yogurt', 'yoga', 'yoenis', 'ymcglaun', 'ykelquiban', 'yiraneuni', 'yikes', 'yield', 'yiayplan', 'yhngsjlg', 'yh', 'yeyeulala', 'yessum', 'yeshayad', 'yennora', 'yemenis', 'yellows', 'yelllowheather', 'yelled', 'yell', 'yehuda', 'yeehaw', 'yeat', 'yeaahh', 'yday', 'ybtheprophet', 'yazidishingalgenocide', 'yard', 'yamashiro', 'yamaguchi', 'yahootv', 'yahoonewsdigest', 'yahoofinancehope', 'yahoocare', 'yahoo7', 'yahistorical', 'yagitudeh', 'yaboiluke', 'xylodemon', 'xxhjesc', 'xvii', 'xtra1360', 'xrays', 'xpost', 'xoxo', 'xojademarie124', 'xo', 'xmen', 'xmas', 'xl', 'xkdrx', 'xii', 'xhnews', 'xgninfinity', 'xfiles', 'xeni', 'xela', 'xekstrin', 'xdojjjj', 'xdescry', 'xboxone', 'xb1', 'xaviermarquis', 'xavier', 'x37bs', 'x2', 'x1441', 'x1434', 'x1411', 'x1402', 'x1392', 'x1386', 'w\\x89Û', 'wzbt', 'wyrmwood', 'wyou', 'wyattb23', 'wxky', 'wxii', 'wxiatv', 'wwwbigbaldhead', 'www', 'wwp', 'wwexdreamer', 'wwa', 'ww3', 'ww', 'wut', 'wugliness', 'wtwitter', 'wtony', 'wth', 'wtc', 'wsvr1686b', 'wsoc', 'wsoaring', 'wsls', 'wsjthinktank', 'wsj', 'wsazbrittany', 'wroug', 'wrote', 'wrongway', 'wrongperson', 'wrongdejavu', 'wrked', 'writingtips', 'writers', 'writebothfists', 'wristband', 'wrightsboro', 'wrestler', 'wrestleon', 'wreak', 'wrapped', 'wrap', 'wraith', 'wr', 'wqow', 'wpt994', 'wps', 'wpo', 'wowthe', 'wowsavannah', 'wout', 'wounded\\x89Û\\x9d', 'woundedpigeon', 'wouldelectrocute', 'worthless', 'worstoverdose', 'worstever', 'worship', 'worsen', 'worseits', 'worrying', 'worm', 'worldwatchesferguson', 'worldvision', 'worldpay', 'worldoil', 'worldnetdailyhomosexuality', 'worldlets', 'workspace', 'workd', 'wordpressdotcom', 'wordk', 'wording', 'wooooooo', 'woodward', 'woodland', 'wooden', 'woodchucks', 'woo', 'wonders', 'wonderousallure', 'wonderkid', 'wonderfully', 'wompppp', 'women\\x89Ûª', 'womengirls', 'womem', 'womb', 'woman\\x89Ûªs', 'wolter', 'wolforth', 'woken', 'woes', 'woah', 'wnw', 'wnukes', 'wniagospel', 'wn', 'wmiddle', 'wm', 'wld', 'wlandslide', 'wkrn', 'wknd', 'wk', 'wizard', 'wiwnpfxa', 'witnessing', 'witnesses', 'witnessed', 'withåÊannihilation', 'withstand', 'withering', 'withdrawur', 'withdraws', 'witch', 'wishlist', 'wishing', 'wished', 'wisely', 'wise', 'wisdomwed', 'wisdom', 'wisdc', 'wipes', 'winnipeg', 'winning', 'winner', 'winik', 'wingers', 'winged', 'wing', 'winechat', 'windwakerstyle', 'windstormåÊinsurer', 'windstormfollow', 'windsor', 'windows10', 'windowgatribble', 'windmy', 'windits', 'win10', 'wimp', 'wimbledon', 'wilsons', 'willow', 'willis', 'willinghearted', 'willieami', 'willian', 'willhillbet', 'wildwestsixgun', 'wildlooking', 'wildlionx3', 'wildlife', 'wildhorses', 'wilden', 'wikipedia', 'wii', 'wifi', 'wifekids', 'wiedemer', 'width', 'widout', 'wider', 'widda16', 'wickett', 'whyor', 'whvholst', 'who\\x89Û', 'whopperjr760', 'whod', 'whocares', 'whitt', 'whitewashes', 'whistled', 'whistleblower', 'whiskey', 'whippenz', 'whipe', 'whimsy', 'while\\x89Û', 'whereas', 'wher', 'whensoever', 'whelen', 'wheelsio', 'wheel', 'whedonesque', 'wheatley', 'whatevs', 'whatever', 'whatcanthedo', 'whashtag', 'whackamole', 'wha', 'wfries', 'wfaaweather', 'we\\x89Ûªve', 'we\\x89Ûªre', 'wexler', 'wews', 'wew', 'west\\x89Ûªs', 'westward', 'wests', 'westminister', 'westmarch', 'westerosnah', 'westeros', 'westerncanadadrought', 'westchester', 'wesleylowery', 'wesley', 'wereonadesolateplanet', 'weren\\x89Ûªt', 'wenger', 'wendell', 'welshninja87', 'weloveyoulouis', 'weloverobdyrdek', 'welovela', 'wells', 'wellknown', 'wellgrounded', 'welles7', 'welladjusted', 'welcomes', 'weirdo', 'weiqin', 'weights', 'weightless', 'weigh', 'weep', 'weeklong', 'weekends', 'weei', 'weebly', 'wednesday\\x89Û', 'wednes', 'wedneday', 'wedgie', 'weddinghour', 'websites', 'webinar', 'web', 'weatherstay', 'weatherit', 'wears', 'weaponxmusic', 'wealilknowa', 'weaknesses', 'weakness', 'wdyouth', 'wdym', 'wdtv', 'wderailed', 'wcw', 'wctv35', 'wccorosen', 'wc', 'wbu', 'wbre', 'wbcshirl2', 'wbc2015', 'waziristan', 'way\\x89Û\\x9dyeah', 'wayward', 'waynesteratl', 'wayi', 'wayfieldstone', 'wave\\x89Ûª', 'waved', 'wattys2015', 'wattle', 'water\\x89Û', 'watersafety', 'waterproof', 'waterfur', 'waterboarding', 'watchthevideo', 'watchout', 'watchin', 'watches', 'wasting', 'wastenoxious', 'wastelands', 'wasted', 'wasn\\x89Ûªt', 'wasnamp8217t', 'wasilla', 'washing', 'washard', 'waseembadami', 'war\\x89Û', 'warzone', 'wartime', 'warthen', 'warsgoddess', 'warriorcord', 'warranted', 'warra', 'warped', 'warningwild', 'warnings900037', 'warnerrobins', 'warned\\x89Û\\x9d', 'warmth', 'warmbodies', 'warlordqueen', 'warfare', 'wardens', 'ward', 'wantmyabsback', 'wanother', 'wank', 'wander', 'waltdisney', 'wall\\x89Û\\x9d', 'wallybaiter', 'wales', 'wakho', 'wakeupflorida', 'waiver', 'waited', 'waimea', 'wahpeton', 'wahhabism', 'wage\\x89Ûª', 'waferthin', 'wackos', 'wackoes', 'vÌdeo', 'v\\x89Û', 'vzwsupport', 'vvorm', 'vuzuhustle', 'vulnerability', 'vulnera', 'vuln', 'vtc', 'vroman', 'vra50\\x89Û\\x9d', 'votes', 'voters', 'vosloorus', 'vortex', 'voortrekker', 'voodooben', 'volunteers', 'volunteer', 'voltaire', 'volfan326', 'volcanotornado', 'volcanodiscover', 'volcanic', 'void', 'voices', 'vodka', 'vocals', 'vocalist', 'vocal', 'vmas', 'vj44', 'vi\\x89Û', 'vixstuart', 'vixmeldrew', 'vivid', 'vivianunhcr', 'viviangiang', 'vivian', 'vivaargentina', 'vitesse', 'vitaly', 'vitalvegas', 'vita', 'visting', 'visiting', 'visionzero', 'visible', 'visibility', 'visage', 'virtual', 'vir', 'viper', 'violin', 'violets', 'violentfeminazi', 'violators', 'violation', 'violated', 'vinustrip', 'vinnie', 'vines', 'vincent', 'vimvith', 'vimeo', 'villicanaalicia', 'villa', 'vilelunar', 'vikings', 'vigils', 'vigilent', 'views', 'viennabutcher', 'videogame', 'videoclip', 'victorious', 'victorias', 'victorian', 'victoriagittins', 'vickybrush', 'vichardy', 'vibrates', 'vibrate', 'viab', 'vhull', 'vhs', 'vgbootcamp', 'vets78734', 'vestment', 'vessels', 'versus', 'veronicadlcruz', 'vern', 'vermilion', 'verhoek', 'vergil', 'verge', 'verde', 'venture', 'ventilated', 'vent', 'venoms', 'veneto', 'veldfest', 'veld', 'veins', 'veil', 'veggies', 'vegetables', 'vegassolitude', 'vegan', 'veg', 'vector', 'vaxshill', 'vast', 'vassalboro', 'varagesale', 'vanpoli', 'vannuyscouncil', 'vanishing', 'vanished', 'vanilla', 'vanessas', 'vanessa', 'vandalized', 'vancouveråÊisland', 'vampires', 'value', 'valuations', 'valleywx', 'vallerand', 'valentines', 'vale', 'valdes1978', 'vail', 'vai', 'vaginaorcake', 'vagersedolla', 'vaccines', 'vaccine', 'vacancies', 'vaca', 'vabengal', 'v452', 'u\\x89Û', 'ux', 'uvopwz', 'uve', 'uv', 'utv', 'utp', 'utopian', 'utilized', 'utility', 'utica', 'utfire', 'utd', 'utc3km', 'utc20150806', 'utahgrizz', 'utahcanary', 'uswarcrimes', 'usw', 'uspacific', 'usmnt', 'ushanka', 'usg', 'usfs', 'useless', 'usdot', 'usbush', 'usatodaynfl', 'usat', 'usarmy', 'usamisan', 'usama', 'usagi', 'usage', 'urufusanragu', 'uruan', 'urself', 'urogyn', 'urine', 'uriminzok', 'urgentthere', 'urg', 'urbanisation', 'urbanfashion\\x89Û', 'urban', 'uranium', 'upwindstorm', 'upward', 'uptownjorge', 'uptown', 'uptotheminute', 'upstairs', 'upsetting', 'uprooting', 'uprootin', 'uplifting', 'upi', 'uphill', 'upgrades', 'updateme', 'upcoming', 'upah', 'upa', 'unwomen', 'unwarranted', 'unwanted', 'unu', 'unto', 'untill', 'untameddirewolf', 'unsurprisingly', 'unsure', 'unsuccessful', 'unstoppable', 'unstable', 'unsigned', 'unsensibly', 'unsecured', 'unrecognized', 'unrealtouch', 'unreal', 'unprepared', 'unpredictable', 'unplug', 'unpacked', 'unnewsteam', 'unloads', 'unlicensed', 'unknowingly', 'univsfoundation', 'universityoflaw', 'uniteblue', 'unite', 'uninvestigated', 'unimpressed', 'unimaginable', 'unhinged', 'unhealed', 'unharmed', 'unhappy', 'ungodly', 'unfortunemelody', 'unfollow', 'unfold', 'unfml', 'unfair', 'unexplainable', 'unending', 'undone', 'undetected', 'undeserving', 'underwritersenior', 'underwriter', 'understood', 'understand\\x89Û\\x9d', 'understandable', 'underpasses', 'undermined', 'undergroundrailraod', 'undergroundbestsellers', 'underground', 'underestimate', 'uncover', 'uncontrolled', 'uncontrollable', 'unconsciously', 'unconscious', 'unconditional', 'uncommon', 'uncomfortable', 'uncles', 'uncertaintyeconomic', 'uncertain', 'unbelievably', 'unaware', 'unauthorized', 'unarmed', 'unaddressed', 'unable', 'umntu', 'umm', 'umbrella', 'um', 'uluru', 'ultimatum', 'ullman', 'uk\\x89Ûªs', 'ukraines', 'uknews', 'ukfrance', 'ukfloods', 'uhmmmm', 'uhhhhh', 'uglypeople', 'uglyamesocialaction', 'ugliest', 'ugc', 'ufo4ublogeurope', 'ufn', 'udom', 'udhampuragain', 'uchicago', 'uabstephenlong', 'u2', 'tÌüp', 't\\x89Û', 'tyrone', 'tyrant', 'typography', 'typing', 'typical', 'typhoon\\x89Û', 'typewriter', 'tyleroakley', 'tyar', 'txt', 'txlege', 'tx', 'twx', 'twoptwips', 'twoout', 'twitsandiego', 'twitch', 'twist', 'twill', 'twilights', 'twi', 'twentysix', 'tweetstorm', 'tweetinglew', 'tweeting', 'tweeted', 'tweet4taiji', 'tween', 'twcnews', 'twain', 'tvshowtime', 'tvjnews', 'tutorials', 'tusky', 'turner', 'turnedonfetaboo', 'turdnado', 'turbojet', 'tunisian', 'tunisia', 'tunis', 'tuneswgg', 'tuned', 'tunas', 'tumblr', 'tumbling', 'tumbles', 'tulowitzki', 'tullamarine', 'tuicruises', 'tuffers', 'tuesdays', 'tucson', 'tub', 'tu', 'ttw', 'tthe', 'tsutomi', 'tsunamis', 'tsipras', 'tshirts', 'tshirt', 'trynna', 'truthsof', 'trustymclusty', 'trusting', 'trusted', 'trulystings', 'truestory', 'truediagnosis', 'truckload', 'truckcrash', 'tru', 'trpreston01', 'trp', 'troyslaby22', 'troylercraft', 'troye', 'troupe', 'troubleonmymind', 'trophyhunt', 'trophy', 'trophies', 'tropes', 'trooper', 'trombonetristan', 'trollkrattos', 'trollingtilmeekdiss', 'troisrivieres', 'trjdavis', 'trixiedrowned', 'trivium', 'triumphs', 'triumphant', 'triumph', 'trinna', 'trim', 'trillac', 'trigger', 'trident', 'tricycle', 'tricky', 'trickxie', 'trickshot', 'trickier', 'triciaoneillphoto', 'triciaoneill', 'tribez', 'tribe', 'trib', 'triangle', 'triad', 'treyarch', 'trey', 'trestle', 'trend\\x89Û\\x9d', 'trends', 'trending', 'trench\\x89Û', 'tren', 'tremors', 'tremont', 'tremblayeh', 'trekkers', 'treescape', 'treblinka', 'treatmen', 'treasures', 'treasurehouse', 'trc', 'tray', 'travellers', 'travelelixir', 'trash', 'traplord29', 'transwomen', 'transportation', 'transporta', 'translated', 'transgress', 'transgendered', 'transgender', 'transcription', 'trampling', 'traitor', 'traintragedy', 'trained', 'trailheads', 'trailed', 'trail', 'trafficnetwork', 'traditionalist', 'trader', 'trade', 'tradcatknight', 'tracy', 'tractor', 'tracklist', 'tracey', 'trace', 'tra', 'tps', 'tprimo24', 'to\\x89Û', 'tozlet', 'toyota', 'toxicsavior', 'toxiccancerdiseasehazardous', 'toxic', 'towns', 'towing', 'tower\\x89Ûª', 'tower', 'towboat', 'tow', 'tours', 'tournaments', 'tournament', 'tourists', 'tounge', 'toughens', 'touching', 'touchdown', 'tottenham', 'totteham', 'totoooooooooo', 'totoooooo', 'totalitarianism', 'totalitarian', 'tosu', 'toss', 'tos', 'tory', 'tort', 'torso', 'torrential', 'torrent', 'torrecilla', 'torrance', 'torontorc', 'tornadogiveaway', 'tormented', 'tories', 'tora', 'topic', 'topdown', 'top25', 'tootrue', 'toosoon', 'toooooo', 'tookitlikeaman', 'tookem', 'toocodtodd', 'tonysandos', 'tonymcguinness', 'tonyhsieh', 'tonycottee1986', 'tonyburke', 'tonyabbottmhr', 'tony', 'tonne', 'tonight\\x89Ûªs', 'tonguetwister', 'ton', 'tomorrow\\x89Ûªs', 'tommorow', 'tomlinson', 'tomislav', 'tomfromireland', 'tomdean86', 'tomclancy', 'tolled', 'tolewantg', 'tolerated', 'tolerance', 'tokteacher', 'toilets', 'toiindianews', 'togthe', 'toes', 'toenail', 'toe', 'toddyrockstar', 'toddstarnes', 'toddcalfee', 'today\\x89Ûªs', 'todaythat', 'todayngr', 'todayng', 'todayim', 'todayhave', 'today4got', 'tod', 'tobiasellwood', 'tnwx', 'tnn', 'tneazzy', 'tna', 'tn', 'tms7', 'tmake', 'tlvfacesauspol', 'tlvfaces', 'tloz', 'tlk', 'tkyonly1fmk', 'tjrobertson2', 'tj', 'titty', 'tittie', 'titortau', 'titolo', 'titania', 'titadom', 'tita', 'tirelessly', 'tire', 'tipster', 'tip', 'tinybaby', 'tiny', 'tinted', 'ting', 'tindering', 'tinderbox', 'tinder', 'timmicallef', 'timing', 'time\\x89Û\\x9d', 'timesofindia', 'timesap', 'timed', 'timebomb', 'timber', 'timaroberts', 'tightly', 'tight', 'tiggr', 'tigersjostun', 'tiffanyfrizzell', 'tier', 'tide', 'tidalhifi', 'ticklemeshawn', 'tianta', 'th\\x89Û', 'thursdays', 'thursd', 'thurs', 'thurlow', 'thunderstormtornado', 'thundersnow', 'thugging', 'thucydiplease', 'tht', 'thruuu', 'thrusts', 'throwin', 'thriving', 'threesome', 'threealarm', 'threat\\x89Ûª', 'threatintel', 'threatconnect', 'thread', 'thrarchives', 'thr', 'thoutaylorbrown', 'thoughwill', 'thou', 'thoroughly', 'thorium', 'thorins', 'thorgan', 'thomassmonson', 'thomashcrown', 'thnk', 'thi\\x89Û', 'this\\x89Û', 'thisispublichealth', 'thisisperidot', 'thisishavehope', 'thisisfaz', 'thisdayinhistory', 'thirtyfive', 'thirdquarter', 'thinner', 'thinkpink', 'thingsihate', 'thh', 'they\\x89Ûªd', 'thexfiles201days', 'thewesterngaz', 'thetxi', 'thetshirtkid', 'thetimepast', 'thestrain', 'thesmallclark', 'thesewphist', 'thesensualeye', 'theresmorewherethatcamefrom', 'thereof', 'thereisonlysex', 'therein', 'therefore', 'therealrittz', 'thereal', 'theramin', 'thepartyofmeanness', 'then\\x89Û', 'thenissonian', 'thenewshype', 'theneeds', 'themhe', 'themermacorn', 'theme', 'themalemadonna', 'themaine', 'thelegendblue', 'thejonesesvoice', 'thejenmorillo', 'theirs', 'thehobbit', 'thehammers', 'thegreenparty', 'theghostparty', 'theevilolives', 'theemobrago', 'theellenshow', 'theeconomist', 'thedoolinggroup', 'thedayct', 'thedarktower', 'thedailyshow', 'thedailybeast', 'thecomedyquote', 'theburnageblue', 'thebriankrause', 'theboyofmasks', 'thebookclub', 'theblackshag', 'thebargain', 'thebachelorette', 'theatres', 'theatlantic', 'theatershooting', 'theashes', 'theadvocatemag', 'thda', 'that\\x89Û', 'thatwitchem', 'thatswhatfriendsarefor', 'thatsabinegirl', 'thatrussianman', 'thatpersianguy', 'thatnot', 'thatfatguy', 'thatdes', 'thatd', 'thankyou', 'thanku', 'thankkk', 'thanking', 'thalapathi', 'thailand', 'thai', 'tfw', 'tfb', 'te\\x89Û', 'texture', 'texts', 'texting', 'texaschainsawmassacre', 'texans', 'testy', 'testify', 'testified', 'testicles', 'tesco', 'terwilliger', 'tersestuff', 'territory', 'terrified', 'terrific', 'terrain', 'termn8r13', 'terminated', 'terell', 'tepat', 'tents', 'tenshi', 'tennis', 'tennews', 'tends', 'ten4', 'tem\\x89Û', 'temps', 'temporary', 'temporarily', 'templates', 'temperature', 'temp', 'temecula', 'temecafreeman', 'telnet', 'tellyfckngo', 'tellyampi', 'telly', 'telltales', 'teleported', 'telemarketing', 'telekinesis', 'telegraphworld', 'telangana', 'tee\\x89Û', 'teenfiction', 'teena797', 'teemo', 'tee', 'teduka', 'tedcruz2016', 'tecno', 'techniqu', 'technical', 'technews', 'techesback', 'team\\x89Û', 'teamvodg', 'teamsurvivors', 'teamscorpion', 'teamo', 'teammates', 'teamhennessy', 'teamfollowback', 'teamatowinner', 'teahivetweets', 'teafrystlik', 'tdog', 'tdm', 'tcotåÊccot', 'tcgreno', 'tbs', 'tbr', 'tblack', 'tbh', 'taylorswift13', 'taylors', 'taykreidler', 'tayiorrmade', 'taxstone', 'taxreturn', 'taxpayers', 'taxis', 'taxi', 'taxes', 'tawfmcaw', 'taungbazar', 'taufikcj', 'tattoos', 'tattooed', 'tat', 'tastemycupcakee', 'tasks', 'task', 'tarzana', 'tarynel', 'tarp', 'targe', 'tareksocal', 'taraswart', 'tapas', 'taoistinsight', 'tantrums', 'tanstaafl23', 'tanslash', 'tangletalk', 'tangled', 'tanehisicoates', 'tampons', 'tampabay', 'tambourine', 'tambo', 'tallest', 'talkradio', 'talkinghell', 'talked', 'talkecologyamphuman', 'talisman', 'talibans', 'taliban', 'tales', 'takis', 'takin', 'takeoff', 'takehome', 'takecare', 'takeaways', 'taipei', 'tailor', 'tail', 'tahoeblazeravalanches10', 'tagging', 'tagged', 'tafs', 'tae', 'tadhgtgmtel', 'tactics', 'tactful', 'tacos', 'tackettdc', 'tacit', 'taaylordarr', 't1000s', 's\\x89Ûªarabia', 'szuter', 'szmnextdoor', 'systematic', 'sys', 'syringetoanger', 'synapsenkotze', 'symptoms', 'symbol', 'symantec', 'syjexo', 'sydtraffic', 'swtrains', 'sworn', 'swords', 'swooping', 'swollen', 'swivels', 'switzerland', 'switching', 'switch', 'swiss', 'swingman', 'swiming', 'swiftycommissh', 'swiftly', 'swellyjetevo', 'swell', 'sweets', 'sweetpeas', 'sweetiebirks', 'sweeps', 'sweeping', 'sweep', 'swedish', 'sweaty', 'sweatfyi', 'sweated', 'swea', 'swb1192', 'swayoung01', 'swayback', 'swanger', 'swami', 'swag', 'svetlana', 'sux', 'suvs', 'sustainourearth', 'sustainability', 'suspicious', 'suspended', 'suspects', 'susiya', 'susinesses', 'sushi', 'susanj357', 'survivorsr', 'surviving', 'surveys', 'suruÌ¤', 'surgical', 'surges', 'surfspa', 'surfphoto', 'surety', 'sureshpprabhu', 'suresh', 'suregod', 'supremo', 'supremacist', 'supposedly', 'supporting', 'supporthealthhomebathroomsupportelderlyinjureds\\x89Û', 'supporters', 'superv', 'superstitions', 'superstition', 'superpower', 'supernovalester', 'supernatural', 'supermarket', 'superman', 'superiority', 'superintendent', 'superintende', 'superfood', 'superbug', 'superb', 'sunshine', 'suns', 'sunrays', 'sunnymeade', 'sunk\\x89Û1', 'sunflower', 'sundercr', 'sunday\\x89Ûªs', 'sundaydont', 'sunburst', 'sunburned', 'sunbathe', 'sumo', 'sumn', 'summons', 'summon', 'summervibes', 'summers', 'summerhallery', 'summer2k15', 'summary', 'sultry', 'sul', 'suites', 'suited', 'suitable', 'suing', 'suicides', 'suicidebycop', 'suho', 'suggs', 'sugar', 'suffield', 'sufficiently', 'suffers', 'suelinflower', 'sued', 'sudan\\x89Ûªs', 'sucking', 'suckers', 'sucked', 'suck', 'succeed', 'subtornado', 'subtlety', 'subtle', 'substantial', 'substance', 'subsequent', 'subsd', 'subscription', 'submitt', 'submissions', 'submerged', 'subjected', 'subcontractor', 'subconscious', 'subcommittee', 'subatomic', 'su', 'stylist', 'stylishly', 'styled', 'stvmlly', 'stury', 'sturgis', 'stupidniggr', 'stunningly', 'stung', 'stunckle', 'stump', 'stuffin', 'studying', 'studebaker', 'studded', 'stuckinbooks', 'stuartbroad8', 'strutting', 'strutted', 'struggle', 'structuring', 'strongminded', 'strives', 'striptease', 'strips', 'stripped', 'stripe', 'strip', 'striked', 'strictly', 'strickskin', 'stretches', 'stretcherbearers', 'stretcherbearer', 'stretched', 'stressing', 'stresses', 'strength', 'streetlight', 'streetjamzdotnet', 'stree', 'streamyxhomesouthern', 'streams', 'streaming', 'stray', 'strawberrysoryu', 'strawberries', 'stratford', 'strategyhua', 'strategies', 'strap', 'strangers', 'strand', 'straits', 'strains', 'strain', 'straighten', 'sto\\x89Û', 'stowing', 'stormtrooper', 'stormlike', 'storming', 'stormfree', 'stormbeard', 'storey', 'storen', 'stop\\x89Û\\x9d', 'stopping', 'stoponesounds', 'stopharper', 'stopevictions', 'stood', 'stony', 'stonewall', 'stones', 'stonebrewingco', 'stokes', 'stockwell', 'stockholm', 'sto', 'stlouis', 'stlnd', 'stl', 'stirring', 'stil', 'stiiilo', 'stickynyc', 'sticky', 'sticking', 'sticker', 'sthing', 'stfxuniversity', 'stevie', 'steveycheese99', 'stevenrulles', 'stevenontwatter', 'steven', 'sterotypical', 'stern', 'sterlingscott', 'sterlingknight', 'sterling', 'stepkans', 'stephenson', 'stephenscifi', 'stephenking', 'stephengeorg', 'stephaniemarija', 'steph93065', 'stemming', 'stem', 'stefano', 'stefanejones', 'steep', 'steellord', 'stearns', 'stealth', 'steady', 'stds', 'staying', 'stayed', 'stavola', 'stavernise', 'statistically', 'stations', 'stationcdrkelly', 'states\\x89Û', 'statesville', 'statements', 'stat', 'starving', 'starve', 'startrek', 'startide', 'startelegram', 'starmade', 'starks', 'stark', 'starflamegirl', 'stare', 'stardate', 'starbuckscully', 'starbs', 'stankyboy88', 'standup', 'standstill', 'standforwolves', 'standardised', 'standardanonymous', 'stallion150', 'stalled', 'stalins', 'stalag', 'staining', 'staid', 'stages', 'staged', 'staff\\x89Û', 'staffing', 'stacy', 'stacks', 'stack', 'stacedemon', 'stacdemon', 'stable', 'ssw', 'ssu', 'ssssnell', 'ssshhheeesshh', 'ssp', 'ssb4', 'srsly', 'srs', 'srk', 'sriramk', 'srajapakse', 'sr37', 'sr22', 'sr14', 'sqwizzix', 'squirrel', 'squibby', 'squeezed', 'squeaver', 'squeaky', 'squabble', 'sq', 'spyro', 'spying', 'spx', 'sputtering', 'sputnikint', 'spurs', 'spurgeon', 'spur', 'spså¨', 'spsgsp', 'sprite', 'sprinklers', 'springer', 'spree', 'spreads', 'spread', 'spray', 'sprains', 'spouting', 'spouse', 'spotting', 'spotlight', 'sportsroadhouse', 'sportinggoods', 'sporten', 'spookyfob', 'sponsor', 'sponge', 'spokes', 'spoke', 'spokane', 'spoiled', 'spoil', 'splifs', 'splattershot', 'splatoon', 'splatling', 'splatdown', 'splash', 'spits', 'spit', 'spirits', 'spins', 'spinnellii', 'spin', 'spilt', 'spillevacuationsred', 'spike', 'spies', 'spiderweb', 'spicybreads', 'spice', 'spends', 'spencerfearon', 'spen', 'spells', 'spell', 'speedtech', 'speeding', 'speech', 'speculation', 'speculatio', 'spectrum', 'specs', 'specifically', 'specialized', 'specialize', 'speccy', 'speakingfromexperience', 'sparxxx', 'spartans', 'sparkz', 'sparking', 'spark', 'spaniels', 'spaniel', 'span', 'spammers', 'spam', 'spacex', 'spacewolverine', 'spaceshiptwo', 'spaceangelseven', 'so\\x89Û', 'soz', 'sow', 'south\\x89Û', 'southwestern', 'southwest', 'southridgelife', 'southline', 'southkorea', 'southdown', 'southbound', 'southaccident', 'sousse', 'sourmashnumber7', 'sour', 'soundtrack', 'sounding', 'sounders', 'soultech', 'sought', 'souda', 'sothwest', 'sos', 'sorryi', 'sorrybutitstrue', 'sorrowful', 'sorrower', 'sorrow', 'sorely', 'sophistication', 'sophiewisey', 'sophieingle01', 'soonpandemonium', 'soonergrunt', 'sonyprousa', 'sony', 'sonoranrattler', 'sonofbobbob', 'sonofbaldwin', 'sonisoner', 'sonia', 'soni', 'songhey89', 'songfor', 'soner', 'sond', 'sona', 'somme', 'sometimesi', 'somethin\\x89Ûª', 'somethingyr', 'someday', 'solving', 'solelinks', 'sole', 'soldi', 'solano', 'sojapan', 'soil', 'soggy', 'softball', 'sofa', 'sods', 'sodamntrue', 'sockets', 'socket', 'sock', 'socialwots', 'socialtimes', 'socialmediadriven', 'socially', 'socal', 'soc', 'sobbing', 'soapscoop', 'soap', 'soaker', 'soaked', 'snuck', 'snowywolf5', 'snowy', 'snowstormhailstorm', 'snowstormdespite', 'snowflake', 'snotgreen', 'snort', 'snoop', 'snooker', 'snippets', 'snipe', 'sniiiiiiff', 'sniff', 'sni', 'sneezing', 'sneaks', 'snazzychipz', 'snapping', 'snapharmony', 'snapchatselfie', 'snakes', 'snake', 'snack', 'sn', 'smusx16475', 'smugglersåÊnabbed', 'smugglers', 'smug', 'smth', 'sms087809233445', 'sms', 'smp', 'smores', 'smoothed', 'smoochy', 'smokey', 'smokes', 'smokers', 'smoakqueen', 'smirking', 'smiles', 'smfh', 'smem', 'smelltaste', 'smelling', 'smelled', 'smeared', 'smartteks', 'smartnews', 'smart', 'smantibatam', 'smallforestelf', 'smaller', 'smallbusiness', 'smallbiz', 'smack', 'slums', 'slumber', 'slsp', 'slsandpet', 'slows', 'slowpoke', 'slosheriff', 'slosher', 'slopeofhope', 'slone', 'slogan', 'slit', 'slipping', 'slipper', 'slimebeast', 'slightest', 'slight', 'sliding', 'slideshare', 'sliced', 'slew', 'slept', 'sleepjunkies', 'slay', 'slaves', 'slavery', 'slave', 'slaughter', 'slatukip', 'slating', 'slated', 'slashandburn', 'slapping', 'slander', 'slams', 'slamming', 'slam', 'slain', 'slabs', 'sl', 'skyåÊnews', 'skywars', 'skywarn', 'skyscrapers', 'skyrim', 'skype', 'skynews', 'skynet', 'skyler', 'skylanders', 'skull', 'skippy6gaming', 'skip', 'skinless', 'skims', 'skimmed', 'skiing', 'skies', 'ski', 'skh', 'sketchbook', 'skeleton', 'skc', 'skateboards', 'skarletan', 'skardu', 'skaggs', 'sk398', 'sjubb', 'sj', 'sizygwwf', 'sizewell', 'sixcar', 'sivan', 'situ', 'sittway', 'siteinvestigating', 'sister\\x89Ûª', 'sirtophamhat', 'sirtitan45', 'sirmixalot', 'sirmione', 'sirius', 'sirenvoice', 'sirens\\x89Û', 'sirensong21', 'sirensamp', 'sippin', 'sip', 'siouxland', 'siouxlan', 'sioux', 'sins', 'sinkingshipindy', 'sinkingfund', 'sinkhole\\x89Û', 'sinistras', 'singled', 'singlecar', 'sindh', 'sincerely', 'since3g', 'since1970the', 'simulation', 'simulating', 'simplify', 'simmons', 'similar', 'silvery', 'silverwood', 'silverman', 'silverhusky', 'silo', 'silly', 'silinski', 'silentmind', 'silent0siris', 'silenced', 'silas', 'sikh', 'sigue', 'signin', 'significance', 'signatureschange', 'sigh', 'sifting', 'siena', 'sidjsjdjekdjskdjd', 'side\\x89Û', 'sidewalk', 'sidelinesavage', 'sided', 'sick\\x89Ûª', 'sibling', 'shuts', 'shunichiro', 'shuffled', 'shuffle', 'shud', 'shtf', 'shtap', 'sht', 'shrews', 'showwent', 'showersstorms', 'showers', 'showdown', 'shovel', 'shove', 'shoutout', 'shouted', 'shouout', 'shotgun', 'shorts', 'shortfalls', 'shopping', 'shoppe', 'shootoutåÊ', 'shootings', 'shoook', 'sholt87', 'shocking\\x89ÛÏ', 'shocking', 'shoalstraffic', 'shiver', 'shitton', 'shite', 'shirley', 'shipsxanchors', 'shimmyfab', 'shii', 'shifts', 'shifter', 'shifted', 'shield', 'shidddd', 'shias', 'shia', 'she\\x89Ûªs', 'shevlinhixon', 'shestooyoung', 'sherfield72', 'shen', 'shemesh', 'sheltersupport', 'shells', 'shekhargupta', 'sheeting', 'shedid', 'shear', 'shayoly', 'shawie17shawie', 'shattered', 'shatter', 'sharply', 'sharper', 'shark\\x89Û\\x9d', 'shark', 'sharif', 'sharia', 'shar', 'shaping', 'shaper', 'shapeand', 'shaolin', 'shantaeskyy', 'shantaehalfgeniehero', 'shantaeforsmash', 'shanghai\\x89Ûªs', 'shanaynay', 'shakjn', 'shakingcatching', 'shakespeares', 'shakeology', 'shaken', 'shake', 'shaheed', 'shadows', 'shadowman', 'shadowflame', 'shadowed', 'shade', 'shad', 'sha', 'sgc72', 'sg', 'sfor', 'sfgiants', 'sfa', 'sexydragonmagic', 'sexuality', 'sexist', 'sewing', 'seward', 'sewage', 'severing', 'severely', 'seventies', 'sevenfold', 'sevenfigz', 'setting4success', 'setsuko', 'sethalphaeus', 'setanta', 'sessions', 'session', 'serving', 'servicin', 'servicesft7p7a', 'sergiopiaggio', 'serephina', 'serene', 'serbian', 'seras', 'sequence', 'sequalae', 'septic', 'separation', 'separated', 'senzu', 'sentient', 'sentenced', 'sensory', 'sensorknock', 'sensitive', 'sensei', 'senschumer', 'sensanders', 'sens', 'senfeinstein', 'senators', 'senatemajldr', 'sen', 'seminars', 'semi', 'semasirtalks', 'selmoooooo', 'selmo', 'selfseeking', 'selfpity', 'selfinflict', 'selfesteem', 'selfdestruction', 'selfdelusion', 'selfavowed', 'selects', 'select', 'sel', 'sejorg', 'seizing', 'seize', 'seismicsoftware', 'seismicresistant', 'segment', 'segas', 'sef', 'seeyouatamicos', 'seeweed', 'seemly', 'seemeth', 'seeker', 'seeds', 'seed', 'sedar', 'sedan', 'securing', 'secures', 'securedgt', 'secured', 'sectors', 'sections', 'secondhand', 'seclusion', 'sec', 'seaworld', 'seattletimes', 'seattles', 'seattledot', 'seats', 'seatbelt', 'seasonfrom', 'seashore', 'seas', 'seanhannity', 'seagulls', 'seagull07', 'sd', 'scynic1', 'scwx', 'scum', 'scuf', 'sct014', 'sct012', 'scseestapreparando', 'scrolling', 'scriptettesar', 'screwed', 'screenshot', 'screening', 'screeching', 'screamsdont', 'scratching', 'scratches', 'scraptrident', 'scraped', 'scrambledeggs', 'scouts', 'scout', 'scourgue', 'scourge', 'scotto519', 'scottdpierce', 'scotrail', 'scotiabank', 'scored', 'scorched', 'scofield', 'scmpnews', 'scissor', 'sciencefiction', 'scichat', 'schulz', 'schoolboy\\x89Ûªs', 'scholars', 'schoenfeld', 'schism\\x89Ûª', 'schelbertgeorg', 'scheer', 'scenes', 'scenario', 'scegnews', 'scasualty', 'scaryeven', 'scarlet', 'scariest', 'scarier', 'scandals', 'scandal', 'scam', 'scalpium', 'scaligero', 'scabs', 'sbee', 'sa\\x89Û', 'say\\x89Û\\x9d', 'saynae', 'sayin', 'sayedridha', 'savs', 'savour', 'savior', 'saveti', 'saves', 'saver', 'savedenaliwolves', 'savannahross4', 'savages', 'savagenation', 'saumur', 'sauldale305', 'saudiåÊmosque', 'saudies', 'saudiarabia', 'saturation', 'saturated', 'satoshis', 'satisfying', 'satire', 'satin', 'satellites', 'satans', 'satanaofhell', 'sask', 'sasha', 'sarumi', 'sarniamakchris', 'sarcastic', 'sarahmcpants', 'sarahksilverman', 'sara', 'sapphirescallop', 'san\\x89Ûªa', 'santos', 'santiago', 'santanicopandemonium', 'santaclara', 'sansa', 'sanonofre', 'sanity', 'sanitizing', 'sanitised', 'sang', 'sanfrancisco', 'sanford', 'sanelesstheory', 'sandwich', 'sandunes', 'sandra', 'sanders', 'sandbox', 'sanction', 'sanchez', 'samsung', 'samsmithworld', 'samples', 'sample', 'sammysosita', 'sammy', 'samihonkonen', 'sami', 'samelsamel', 'samaritans\\x89Ûª', 'sam', 'salyersblairhall', 'salvages', 'salvadors', 'salvadoran', 'salute', 'salty', 'saltriverwildhorses', 'salted', 'salopek', 'salmanmydarling', 'salman', 'sally', 'salisbury', 'sales', 'salado', 'saladinahmed', 'sakuuchiha', 'saku', 'sakhalintribune', 'saison', 'saintsfc', 'saint', 'sailors', 'safyuan', 'safsufa', 'safferoonicle', 'safes', 'saferåÊ', 'safeco', 'safaris', 'safari', 'sadtraumatised', 'saddledome', 'saddle', 'sacrifice', 'sackville', 'sackings', 'sabotagei', 'sabcnewsroom', 'saat', 'saalon', 'saadthe', 's61231a', 's5', 's3xleak', 's01e09', 'rzimmermanjr', 'ryt', 'ryrotheunaware', 'ryleedowns02', 'ryans', 'ryanoss123', 'rwrabbit', 'rvfriedmann', 'rvaping101', 'rvacchianonydn', 'rv', 'ruthann', 'russiaukraine', 'russellville', 'russell', 'russaky89', 'rushlimbaugh', 'rural', 'rupaul', 'run\\x89Û', 'runnin', 'runners', 'runkeeper', 'runjewels', 'runin', 'runaway', 'runabout', 'rumor', 'rumbling', 'rumah', 'rum', 'ruling', 'ruler', 'ruled', 'ruhl', 'rude', 'ruddyyyyyy', 'rubybot', 'rubi', 'rubbing', 'rubbin', 'rubbery', 'ru', 'rtsampdemocracy', 'rtrrtcoach', 'rtirishirr', 'rtcom', 'rsx', 'rstormcoming', 'rspca', 'rslm72254', 'rsf', 'rsa', 'rs5', 'rs40000cr', 'rrusa', 'rq', 'rpn', 'rp', 'roy', 'rowysolouisville', 'rowyso', 'rowaa', 'roving', 'routing', 'routine', 'routes', 'router', 'rousey', 'roundhouse', 'round2', 'rotting', 'rottentomatoes', 'rotations', 'rotation', 'rotating', 'rotary', 'rosters', 'roster', 'rossmartin7', 'rossbarton', 'roskomnadzor', 'rosewell', 'rosenthalauthor', 'rosenbergs', 'rosemarytravale', 'rorington95', 'ropes', 'roomsgrrrr', 'rooms', 'roomr', 'rooftops', 'roofing', 'roofers', 'ronwyden', 'ronincarbon', 'ronge', 'rondarousey', 'ronda', 'ronald', 'romp', 'romford', 'romes', 'romeocrow', 'romeo', 'romanticsuspense', 'romantic', 'romania', 'romanatwoodvlogs', 'roman', 'rom', 'rolo', 'rolling\\x89Û', 'roles', 'roleplay', 'rolandonabeats', 'rokiieee', 'roh3smantibatam', 'roguewatson', 'rogers', 'roga', 'rods', 'rodkiai', 'roddypiperautos', 'rodarmer21', 'rockstar', 'rockn', 'rockingham', 'rocking', 'rochdale', 'robthieren', 'robsimss', 'robpulsenews', 'robotlvl', 'robotcoingame', 'robot', 'roblox', 'robertwelch', 'robertoneill31', 'robertmeyer9', 'robertharding', 'robertcalifornia', 'robertbenglunds', 'robdelaney', 'robbiewilliams', 'robbed', 'roar', 'roanoketimes', 'roadworks', 'roadwayproperty', 'roadid', 'rnk', 'rlyeh', 'rlauren83199', 'rjkrraj', 'rjg0789', 'rjailbreak', 'rizzo', 'rivers', 'riverroaming', 'riveeeeeer', 'rivals', 'ritzyjewels', 'ritualistic', 'ritual', 'rite', 'risky', 'riser', 'ris', 'ririnsider', 'rips', 'ripriprip', 'ripples', 'ripping', 'rioters', 'rioslade', 'riooooos', 'rio2016', 'rinkydnk2', 'rindou', 'rin', 'rijn', 'rigour', 'rightly', 'righteous', 'rigga', 'rig', 'rifle', 'ridiculously', 'riddler', 'ricotta\\x89Û', 'rico', 'rickybonessxm', 'rickets', 'ricin', 'richhomeydon', 'riches', 'richelieusaintlaurent', 'richarkkirkarch', 'riceechrispies', 'rice', 'ribbon', 'ri', 'rhymes', 'rhinestone', 'rhiannon', 'rhett', 'rhee1975', 'rgj', 'rfp', 'rfcgeom66', 'rezaphotography', 'reworked', 'rewatchingthepilot', 'revolutionblight', 'revolt', 'revitup', 'revise', 'reviewing', 'reverse', 'reversal', 'revere', 'revenge', 'revel', 'reveillertm', 'revealing', 'reusing', 'reunite', 'retweeted', 'returning', 'retroactive', 'retreat', 'retract', 'retooled', 'retirement', 'retirees', 'retiredfilth', 'retard', 'retainers', 'resumed', 'restrospect', 'restoringpaths', 'restoring', 'restlessness', 'resting', 'restart', 'resque', 'responding', 'respondents', 'respects', 'respecting', 'resource', 'resort', 'resolved', 'resolutevanity', 'resoluteshield', 'resistant', 'resin', 'resilience', 'resigninshame', 'residualincome', 'residual', 'reshrimplevy', 'reshareworthy', 'reshape', 'reset', 'reserves', 'reserved', 'resemblance', 'researchers', 'rescuing', 'rescuersthe', 'rescued\\x89Û', 'rescuedagain', 'rescind', 'requiring', 'requiem', 'requests', 'requa', 'reqd', 'reputation', 'reps', 'reprocussions', 'reprises', 'repression', 'represents', 'representing', 'representative', 'repped', 'reporters', 'replacement', 'replaced', 'repjohnkatko', 'repdonbeyer', 'repatriating', 'reoccur', 'rent', 'renovation', 'renewsit', 'renewed', 'renew911health', 'rene', 'rendered', 'render', 'renaomino', 'renamed', 'ren', 'remymarcel', 'remote', 'remorseless', 'remodeled', 'remixes', 'reminders', 'reminded', 'remind', 'remembrance', 'remembers', 'rememberrabaa', 'remedial', 'rembr', 'remaster', 'remarkably', 'remark', 'remand', 'remainontop', 'remaining', 'remade', 'relive', 'religious', 'reliefweb', 'relevance', 'relegation', 'releasing', 'relaxinpr', 'relations', 'rejoice', 'rejectdcartoons', 'reiterate', 'reined', 'reince', 'reimagining', 'reigncoco', 'reidlake', 'regress', 'regr', 'regime', 'reggaeboyz', 'regent', 'regc', 'reg', 'refuses', 'refunds', 'refund', 'refugees\\x89Û', 'refugeesmatter', 'reflects', 'reflections', 'reflected', 'referred', 'referencereference', 'reference', 'refer', 'reeves', 'reef', 'reed', 'reebok', 'redwing', 'reduces', 'redskins', 'reds', 'redlands', 'redistribute', 'rediscovered', 'rediscover', 'redhead', 'redhanded', 'redesigning', 'redesigned', 'redemption', 'redeemer', 'redeem', 'reddish', 'reddevil4life', 'reddakushgodd', 'redcliffe', 'redbull', 'redblood', 'recruitment', 'recruiting', 'recovered', 'recordhigh', 'recorded', 'recordand', 'reconnect', 'recommendations', 'recoil', 'recognition', 'recognised', 'recluse', 'reckless', 'recipe', 'recip', 'receives', 'recalled', 'recal', 'rec', 'rebound', 'reboot', 'reblogged', 'rebelmage2', 'rebelled', 'rebecca', 'rebahes', 'reassigned', 'realm', 'realliampayne', 'realjaxclone', 'realizations', 'realization', 'realities', 'realistic', 'realhotcullen', 'realhiphop', 'reagans', 'reagan', 'reafs', 'readiness', 'reader', 'reacts', 'reactors', 'reactorbased', 'reactions', 'reached', 'reaad', 'rdg', 'rdconsider', 'rconspiracy', 'rcityporn', 'rchs', 'rbi', 'rbcinsurance', 'razedåÊ\\x89ÛÒ', 'razak', 'rayquazaerk', 'raychielovesu', 'rawfoodbliss', 'ravioliåÊwith', 'rave', 'rationing', 'ratio', 'ratingscategories', 'ratings', 'ratingbut', 'rascal', 'raredealsuk', 'rar', 'raptorsbeg', 'raped', 'rants', 'rantipozi', 'ransacked', 'ranks', 'ranking', 'ranked', 'rank', 'raniakhalek', 'rangerkaitimay', 'rang', 'randy', 'randomtourist', 'randomthought', 'randerson62', 'randallpinkston', 'rams', 'rampage', 'ramp', 'ramat', 'ram', 'ralph', 'rally\\x89Û', 'raisinfingers', 'raishimi33', 'rainy', 'rainwindstorm', 'rainforestresq', 'raineishida', 'railroad', 'railguns', 'raidersreporter', 'rahulkanwal', 'raheelsharif', 'raheel', 'raging', 'rag', 'raft', 'raffirc', 'radychildrens', 'radler', 'radios', 'radioriffrocks', 'radical', 'rachelcaine', 'racer', 'raccoons', 'racco', 'rabidmonkeys1', 'rabbit', 'rabaa', 'raabchar28', 'ra', 'r5live', 'r3do', 'r21', 'r1354', 'qzloremft', 'qz', 'quottelevision', 'quotoperations', 'quotesttg', 'quoteoftheday', 'quora', 'quizzed', 'quit', 'quirk', 'quiet', 'quicker', 'quests', 'questionfatalityflawless', 'questergirl', 'quem', 'queer', 'queenwendy', 'queenswharf', 'queenmy', 'quarter', 'quarrel', 'quantit\\x89Ûhttpstco64cymg1ltg', 'quals', 'qualit', 'quake', 'quadrillion', 'qty', 'qpr1980', 'qotring', 'qnh', 'qew', 'qendil', 'qave', 'qampa', 'q99', 'q13', 'q1', 'python', 'pyrotechnic', 'pyrbliss', 'pyramidhead76', 'pydisney', 'pwhvgwax', 'pvris', 'puts', 'puth', 'pussyxdestroyer', 'pusssssssssy', 'push2left', 'purposely', 'purpose', 'purported', 'purpleturtlerdg', 'purified', 'purely', 'purdies', 'puppyshogun', 'puppet', 'pup', 'puny', 'punk', 'punishing', 'punishable', 'pundits', 'pundit', 'punch', 'pumpkin', 'pumper', 'pumped', 'pummel', 'pulse', 'pull\\x89Ûone\\x89Ûyou', 'pullup', 'pulkovo', 'puledotechupdate', 'pugwash', 'pugprobs', 'pug', 'puff', 'puerto', 'puddle', 'puckflattened', 'publishing', 'publish', 'publicityalthough', 'publichealth', 'pub', 'pu', 'pt4', 'pt1', 'psychrewatch', 'psychologist', 'psychic', 'psp', 'psfda', 'pseudojuuzo', 'psd', 'psalm3422', 'psa', 'ps3', 'ps2', 'ps1', 'prysmian', 'proxy', 'proxies', 'provokes', 'providers', 'provided', 'proven', 'prove', 'proudgreenhome', 'protostates', 'protestors', 'protesters', 'protein', 'protected', 'prosser', 'prosper', 'pros', 'proportions', 'prophets', 'prophecy', 'propertycasu', 'properly', 'propelled', 'propane', 'propaganda', 'pronouncing', 'proms', 'prompting', 'prompt', 'promotion', 'promoted', 'promo', 'promised', 'prom', 'prolong', 'prolly', 'proliferation', 'projectiles\\x89ÛÒ', 'progressives', 'progress4ohio', 'programs', 'profittothepeople', 'profithungry', 'professionally', 'profbriancox', 'productive', 'production', 'produces', 'producer', 'produce', 'produc', 'prodemocracy', 'proc', 'probs', 'probability', 'prob', 'privilege', 'prisonplanet', 'prisoners', 'prints', 'printing', 'printed', 'printable', 'principle', 'princessduck', 'princeoffencing', 'primalkitchen', 'primal', 'priests', 'priest', 'pride', 'prez', 'previews', 'preventative', 'prevalent', 'prettyboyshyflizzy', 'pretenses', 'presume', 'preston\\x89Ûªs', 'prestige', 'presstv', 'presssec', 'pressing', 'presser', 'presley', 'presinkhole', 'president\\x89Û\\x9d', 'presidential', 'preset', 'preserve', 'presents', 'presentation', 'preseasonworkouts', 'preseason', 'preschool', 'presbad', 'preppertalk', 'preppers', 'preparedelectrocutedboiling', 'prensa', 'premises', 'prem', 'preferable', 'prefecture', 'preemptive', 'predynastic', 'predictions', 'preconditioning', 'precisionistic', 'precedent', 'preaching', 'preacher', 'praying', 'prayforsaipan', 'prayed', 'praise', 'pragnik', 'practitioner', 'practicing', 'practicenyg', 'practically', 'prablematicla', 'pra', 'ppsellsbabyparts', 'ppor', 'pple', 'ppfa', 'ppc', 'ppact', 'pp400dr', 'pp15000266858', 'pp15000266818', 'pp', 'poze', 'pozarmy', 'pox', 'powerwow', 'powers', 'powerhiroshima', 'powder', 'poway', 'pow', 'pouring', 'poured', 'pour', 'pounds', 'pounding', 'pounded', 'pouch', 'potter', 'pots', 'potatoes', 'pot', 'postponed', 'postexistence', 'postering', 'poster', 'postcards', 'postcapitalism', 'postal', 'possess', 'possesion', 'positively', 'poses', 'pos', 'portrait', 'portfolio', 'portaloos', 'porno', 'pornhub', 'porcupine', 'porcini', 'population6', 'popeyes', 'popcorn', 'pop2015', 'pony', 'ponting', 'pone', 'polluted', 'pollster', 'politic\\x89Û', 'politicized', 'politicians', 'politely', 'polit', 'policylab', 'police\\x89Û', 'policeng', 'pole', 'polaroids', 'polar', 'pol', 'pokemoncards', 'pokemon', 'poisoned', 'point\\x89Û', 'pointless', 'poignant', 'pogo', 'podcast', 'pod', 'poconorecord', 'pochette', 'poc', 'pneumonia', 'plymouth', 'plumbing', 'plugin', 'plsss', 'plotted', 'ploppy', 'pll', 'plez', 'pletch\\x89Ûªs', 'pledged', 'pleb', 'pleasant', 'pleaded', 'plaza', 'playthrough', 'playstation', 'playoverwatch', 'playingnow', 'playa', 'platt', 'platinum', 'plastics', 'plastic', 'plantcovered', 'plantations', 'planners', 'plannedparenthood', 'plank', 'planing', 'planetary', 'pl', 'pkwy', 'pjcoyle', 'pizzas', 'pizzarev', 'pixelsmovie', 'pixeljanosz', 'pixelcanuck', 'pixar', 'pivot', 'pity', 'pittsburgh', 'pitmix', 'pitcher', 'pitched', 'pit', 'piss', 'pisco', 'pirates', 'pirate', 'piracy', 'pir', 'piprhys', 'piping', 'piperwearsthepants', 'pipeliners', 'pioneer', 'pineview', 'pin23928835', 'pills', 'pilgrims', 'pileup', 'piles', 'pileq', 'pikin', 'pikachu', 'pigeon', 'piga', 'piercings', 'piercing', 'pierce', 'pierc', 'piece\\x89Û', 'pieceofme', 'pictwittercompnpizody', 'pictured', 'picthis', 'pickpocket', 'pickle', 'pickens', 'pianohands', 'piano', 'physician', 'phuket', 'photoop', 'photographs', 'photographed', 'photogenic', 'phnotf', 'phillips', 'phillip', 'philippines\\x89Û', 'philippine', 'philippi', 'philipduncan', 'philip', 'phelimkine', 'pharrell', 'pharma', 'phantasmal', 'phandom', 'phalaborwa', 'ph0tos', 'pga', 'pft', 'pfft', 'pfannebeckers', 'petty', 'petting', 'petitiontake', 'petitionno', 'petersens', 'petersburg', 'peterknox', 'peterhowenecn', 'petereallen', 'peterduttonmp', 'petelmcguire', 'petebests', 'pete', 'petchary', 'peta', 'pet', 'pestle', 'perspectives', 'perspective', 'personnel', 'personalize', 'personalinjury', 'persistent', 'persist', 'perrychat', 'perrybellegarde', 'perrie', 'perpetrators', 'permission', 'periwinkle', 'perished', 'periscope', 'perform', 'perforated', 'perfectly', 'perceive', 'pepperoni', 'peoplegt', 'peoplecommunication', 'pension', 'penny', 'pennlive', 'pennies', 'penneys', 'peninsula', 'penetrate', 'penalty', 'pelosis', 'peice', 'peeters', 'peers', 'peeped', 'peel', 'peeked', 'pee', 'pedro20', 'pediatric', 'pedals', 'peasants', 'pearlharbor', 'pearl', 'peale', 'peace\\x89Ûª', 'peacetime', 'pdxabq', 'pd', 'pci', 'pcaldicott7', 'pbx', 'pbs', 'pbohanna', 'pbcanpcx', 'pb', 'paypile', 'payment', 'paying', 'paydayprison', 'payday', 'payback', 'paxton', 'pawsox', 'paws', 'paulstaubs', 'pauls', 'paulista', 'paulhollywood', 'pattyds50', 'patterns', 'patrol', 'patrickwsls', 'patrickjbutler', 'patriciatraina', 'patna', 'patio', 'patientreported', 'paths', 'pathfinders', 'pat', 'pastures', 'pastor', 'pastie', 'passive', 'passion', 'pascoe', 'pascal', 'partys', 'partners', 'partner', 'parties', 'participating', 'participate', 'partially', 'parter', 'partake', 'parsholics', 'pars', 'parliment', 'parliamentary', 'parley\\x89Ûªs', 'parksboardfacts', 'parks', 'parked', 'parkchat', 'parisian', 'parental', 'parent', 'pardon', 'parched', 'paratroopers', 'paramore', 'paramedics', 'paraguay', 'paradise', 'paracord', 'para', 'papicongress', 'papi', 'paperwork', 'paperback', 'papcrdoll', 'papa', 'pantofel', 'panties', 'panther', 'pantalonesfuego', 'panik', 'panics', 'panicked', 'pandora', 'pandemoniumiso', 'pandemic', 'pancakes', 'panama', 'pampered', 'pampalmater', 'palmoil', 'palmer', 'palm', 'palinfoen', 'palestinian\\x89Û', 'palermo', 'paleface', 'pale', 'pakthey', 'pakistans', 'pajamas', 'paints', 'painthey', 'painful', 'paine', 'paging', 'pageshi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an embedding using a Embedding Layer\n",
        "\n",
        "To make our embedding, we are going to use tensorflow embedding layer\n",
        "\n",
        "refer: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "\n",
        "The parameters we care most about for our embedding layer:\n",
        "* `input_dim` = the size of our vocabulary\n",
        "* `output_dim` = the size of the output embedding vector, for example, a value of 100 would be 100 long,\n",
        "* `input_length` = length of the sequences being passed to the embedding layer"
      ],
      "metadata": {
        "id": "4j_6FIARASgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length, #set input shape\n",
        "                             embeddings_initializer= \"uniform\",\n",
        "                             output_dim=128,\n",
        "                             input_length=max_length #how long is each input\n",
        "                             )\n",
        "\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwodP_LBBIvj",
        "outputId": "43831f61-961f-49f9-c480-ad2ce8292c30"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7fceb0527290>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get a random sentence from the training set\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n {random_sentence}\\\n",
        "      \\n\\nEmbedded version:\")\n",
        "\n",
        "#embed the random sentence (turn it into dense vectors of fixed size)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY_UgCM0CWrI",
        "outputId": "939049a3-cc59-4999-968f-7716634e019f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            " @msnbc What a fucking idiot. He had a gun &amp; a hatchet yet there were still no serious injuries. Glad police terminated him.      \n",
            "\n",
            "Embedded version:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.00576259, -0.02673949, -0.00955553, ..., -0.01188014,\n",
              "         -0.02867581,  0.01594694],\n",
              "        [ 0.02039823, -0.03830385,  0.01551535, ...,  0.00766414,\n",
              "         -0.03295922,  0.0477359 ],\n",
              "        [-0.0210475 , -0.0103291 ,  0.02397022, ..., -0.02447577,\n",
              "          0.00715381, -0.04708336],\n",
              "        ...,\n",
              "        [-0.00387442,  0.0204804 , -0.00559043, ...,  0.00579429,\n",
              "         -0.00050597, -0.01525849],\n",
              "        [-0.0282137 , -0.02687168,  0.03409402, ..., -0.03797891,\n",
              "          0.04715807,  0.00426496],\n",
              "        [-0.01717808,  0.00722102,  0.0213491 , ..., -0.00711459,\n",
              "          0.04759342,  0.0291253 ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check out a singel token's embedding\n",
        "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_TLd4O4C9Bz",
        "outputId": "0ba69586-0e2f-4105-9463-72900b6f4fdc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              " array([-0.00576259, -0.02673949, -0.00955553,  0.04392958, -0.03729944,\n",
              "         0.02506921,  0.02221851,  0.03569968, -0.02726765, -0.01344221,\n",
              "        -0.00478448,  0.0007252 ,  0.04579426,  0.01781093,  0.03366325,\n",
              "         0.02027675, -0.02961365,  0.03061653,  0.00704758,  0.04070561,\n",
              "        -0.02218343,  0.01687001,  0.04613571, -0.01520597, -0.03769933,\n",
              "        -0.041811  , -0.00088688, -0.03568292,  0.00016854, -0.00026079,\n",
              "        -0.0455236 ,  0.04300794, -0.04685124, -0.00428407, -0.01534842,\n",
              "        -0.01122584,  0.0160892 ,  0.00104164, -0.0406921 , -0.02263486,\n",
              "         0.01910459, -0.04194247, -0.02782191,  0.02355717,  0.04549452,\n",
              "        -0.01040699,  0.02576956, -0.00026002, -0.00537946,  0.01686386,\n",
              "        -0.04819901,  0.02957657, -0.00032154, -0.03147017,  0.00252694,\n",
              "         0.01409597, -0.0015525 , -0.02907459,  0.01079339,  0.00531127,\n",
              "        -0.0134807 ,  0.01232969, -0.01816048, -0.01472274, -0.00733678,\n",
              "        -0.00892316, -0.00483803,  0.02254407,  0.00062822,  0.04016386,\n",
              "        -0.04522178, -0.01119536,  0.01196418, -0.00411775, -0.0429813 ,\n",
              "        -0.00121163, -0.03732722, -0.03032572,  0.03430459, -0.0205969 ,\n",
              "        -0.04485775,  0.00560569, -0.00729883,  0.03535135, -0.0102153 ,\n",
              "        -0.00266284, -0.04842671,  0.00527974, -0.0266936 , -0.04075187,\n",
              "         0.0346556 , -0.04584704, -0.01022893,  0.02581917,  0.01213851,\n",
              "        -0.02051389,  0.02451649,  0.03847579, -0.02991765,  0.03487867,\n",
              "        -0.00127038, -0.02409684, -0.01137888, -0.02599237,  0.04961157,\n",
              "        -0.01963713, -0.01433999, -0.02395425,  0.01444686, -0.00196733,\n",
              "         0.02844122, -0.01799607,  0.03086228,  0.01616019, -0.04629115,\n",
              "        -0.02550001, -0.00771325,  0.04198195,  0.0038105 ,  0.01224564,\n",
              "         0.01151574, -0.03336646, -0.02967103, -0.04154998,  0.04414519,\n",
              "        -0.01188014, -0.02867581,  0.01594694], dtype=float32)>,\n",
              " TensorShape([128]),\n",
              " '@msnbc What a fucking idiot. He had a gun &amp; a hatchet yet there were still no serious injuries. Glad police terminated him.')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling a text dataset (running a series of experiments)\n",
        "\n",
        "Now we have got way to turn our text sequences into numbers, it is time to start building a series of modelling experiments.\n",
        "\n",
        "We will start with a baseline and move on from there.\n",
        "\n",
        "* Model 0: Naive Bayes(baseline, this is from sklearn ML map: \n",
        "* Model 1: Feed=forward Neural Network (dense model)\n",
        "* Model 2: LSTM model (RNN)\n",
        "* Model 3: GRU model (RNN)\n",
        "* Model 4: Bidirectional-LSTM model (RNN)\n",
        "* Model 5: 1D Convolutional Neural Network (CNN)\n",
        "* Model 6: Tensorflow Hub pretrained feature extractor (using transfer learning for NLP)\n",
        "* Model 7: same as model 6 with 10% of training data\n",
        "\n",
        "how are we going to approach all of these?\n",
        "\n",
        "Use the standard steps in modelling with tensorflow:\n",
        "* Create a model\n",
        "* Build a model\n",
        "* Fit a model\n",
        "* Evaluate our model"
      ],
      "metadata": {
        "id": "cYp-egarD5VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Getting a baseline\n",
        "\n",
        "As with all machine learning modelling experiments, it is important to create a baseline model so you have got a benchmark for future experiments to build upon.\n",
        "\n",
        "To create our baseline, we will use Sklearn's Multinormial Naive Bayes using the T-IDF ormula to convert our words to numbers.\n",
        "\n",
        ">Note: It's common practice to use non-DL algorithms as a baseline because of their speed and then later using DL to see if you can improve upon them."
      ],
      "metadata": {
        "id": "ThpmP9DzEiTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), #convert words to numebers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) #model the text, \"clf\" stands for classifier\n",
        "\n",
        "])\n",
        "\n",
        "#Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm-h1Izda1mS",
        "outputId": "50fcd0ad-3267-469f-9e2f-0f60dcfff8a9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate our baseline model\n",
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7gNsIbzb2nL",
        "outputId": "e931ed66-9374-4685-9e02-9d08bcde8b72"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our baseline model achieves an accuracy of: 79.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ5MT2lScL4k",
        "outputId": "1a2d75c6-6a7d-420b-b0c6-4465603ea9d4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an evaluation functions for our model experiments\n",
        "\n",
        "We could evaluate all of our model's predictions with different metrics every time,  however this will be cumbersome and could easily fix with a function.\n",
        "\n",
        "Let's create one to compare our model's predicitons with the truth labels using the following metrics:\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-score\n",
        "\n",
        "For a deep overview of many different evalaution methods, see the Sklearn documentation: https://scikit-learn.org/stable/model_selection.html"
      ],
      "metadata": {
        "id": "rYr6LKIJcZVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate model accuracy, precision, recall and 1-core of a binary classification model\n",
        "  \"\"\"\n",
        "\n",
        "  #calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred)*100\n",
        "\n",
        "  #calculate model precision, recall and 1-score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ =precision_recall_fscore_support(y_true,y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                   \"precision\": model_precision,\n",
        "                   \"recall\": model_recall,\n",
        "                   \"f1\": model_f1}\n",
        "  return model_results\n"
      ],
      "metadata": {
        "id": "Vleh2Swsdxbo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get baseline results\n",
        "baseline_results = calculate_results(y_true= val_labels, \n",
        "                                     y_pred= baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMiOWbz6f03W",
        "outputId": "81a9a072-6120-458b-ef13-b1bb3dfb430d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'f1': 0.7862189758049549,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: A simple dense model\n",
        "\n"
      ],
      "metadata": {
        "id": "KIGT2lPTgEAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a tensorboard callback (need to create a new one for each model)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "#create a directory to save TensorBoard logs\n",
        "SAVE_DIR =\"model_logs\""
      ],
      "metadata": {
        "id": "-TePhMRIhxIs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model with the Funtional API\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string) #inputs are 1-dimensional strings\n",
        "x =text_vectorizer(inputs) #turn the input text into numbers\n",
        "x =embedding(x) #create an embedding of the numberized inputs\n",
        "x = layers.GlobalAveragePooling1D(name=\"global_avg_pool_layer\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) #create the output layer, want binary outputs so use sigmoid activation\n",
        "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"
      ],
      "metadata": {
        "id": "nKWF3rfQihxF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQoYiKqtjrzW",
        "outputId": "d6a23739-18e6-4d3e-b3e1-a87b69c63de9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_avg_pool_layer (Glob  (None, 128)              0         \n",
            " alAveragePooling1D)                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile model\n",
        "model_1.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "  "
      ],
      "metadata": {
        "id": "LRGOcBKDkGV5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "model_1_history = model_1.fit(x=train_sentences,\n",
        "                              y=train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences,val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
        "                                                                     experiment_name=\"model_1_dense\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMKWFZyekZ2E",
        "outputId": "9f2b0fda-7cf5-4c27-9711-84aa703e28d6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_1_dense/20220307-063431\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 8s 8ms/step - loss: 0.6118 - accuracy: 0.6974 - val_loss: 0.5351 - val_accuracy: 0.7598\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.4403 - accuracy: 0.8189 - val_loss: 0.4697 - val_accuracy: 0.7848\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.3479 - accuracy: 0.8571 - val_loss: 0.4585 - val_accuracy: 0.7887\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.2848 - accuracy: 0.8923 - val_loss: 0.4651 - val_accuracy: 0.7887\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.2381 - accuracy: 0.9126 - val_loss: 0.4849 - val_accuracy: 0.7887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the results\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0RD7x0v7t5u",
        "outputId": "215a4d00-b748-4af3-f011-8c06307d944d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4849 - accuracy: 0.7887\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.484855979681015, 0.7887139320373535]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make some predictions and evaluate those\n",
        "model_1_pred_probs = model_1.predict(val_sentences)\n",
        "model_1_pred_probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP3tMBc875Sg",
        "outputId": "fe7a1e17-ca2f-4c7b-b52b-bbe84f7e1039"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(762, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GkjNBMC8Sak",
        "outputId": "d4b0ff4f-ffb9-451d-d880-20eaca501424"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.29331428],\n",
              "       [0.7682737 ],\n",
              "       [0.99752814],\n",
              "       [0.08514155],\n",
              "       [0.09632441],\n",
              "       [0.9261592 ],\n",
              "       [0.91517967],\n",
              "       [0.9925121 ],\n",
              "       [0.9592696 ],\n",
              "       [0.22024627]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ht84YU8VeQ",
        "outputId": "a0dfd760-ab45-463a-cfb0-fc9e6777fcfa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_avg_pool_layer (Glob  (None, 128)              0         \n",
            " alAveragePooling1D)                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert  model predictions probs to label format\n",
        "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))"
      ],
      "metadata": {
        "id": "qnm-mRDC8v8E"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7M5SFQH-BsK",
        "outputId": "efc5a9e2-4bc4-4d16-b95e-870743c8ecaa"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate our model_1_results\n",
        "model_1_results = calculate_results(y_true = val_labels,\n",
        "                                    y_pred=model_1_preds)"
      ],
      "metadata": {
        "id": "URbncOQF-dpP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N-pGEsN-9_W",
        "outputId": "a76e76c8-7347-451d-dde9-6eb4073bf3c5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 78.87139107611549,\n",
              " 'f1': 0.7850561936710079,\n",
              " 'precision': 0.7958623979922417,\n",
              " 'recall': 0.7887139107611548}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mk8sAFY_FDz",
        "outputId": "b3c043bc-719e-49f6-fc16-fb916a6daa0d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'f1': 0.7862189758049549,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9MGHBwB_HhU",
        "outputId": "ee878b1a-43a4-48b8-cd4b-32ecaab1fc0a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing learned embeddings"
      ],
      "metadata": {
        "id": "mTCGgqJW_n0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the vocabulary from the text vectorization layer\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "len(words_in_vocab), words_in_vocab[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN6fDqplAKWZ",
        "outputId": "e0169246-17b4-489b-98bd-0c976e5bdc7d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model 1 summary\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8thokx9rAnma",
        "outputId": "c8607168-ea3c-4050-c96e-c05667848cee"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_avg_pool_layer (Glob  (None, 128)              0         \n",
            " alAveragePooling1D)                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the weight matrix of embeding layer\n",
        "#get the numerical representations of each token in our trianing data\n",
        "embed_weights= model_1.get_layer(\"embedding\").get_weights()[0]\n",
        "embed_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJX4CopLAuvr",
        "outputId": "2b3159d2-38d1-41f5-93e6-30548f1e2eb6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.02483202,  0.01841713, -0.04547741, ...,  0.03361918,\n",
              "        -0.00617483,  0.04086307],\n",
              "       [ 0.06777941,  0.00785667, -0.03973348, ..., -0.01383446,\n",
              "        -0.02088076,  0.0439361 ],\n",
              "       [-0.02067542, -0.02631748, -0.01481054, ..., -0.02204605,\n",
              "        -0.01576811, -0.03613602],\n",
              "       ...,\n",
              "       [ 0.03043685,  0.03051025,  0.03076148, ...,  0.04252647,\n",
              "         0.01093336,  0.00094986],\n",
              "       [ 0.0579463 , -0.01734558, -0.0320225 , ...,  0.01959758,\n",
              "        -0.03875774,  0.00202973],\n",
              "       [ 0.01958668, -0.08377369, -0.11549132, ...,  0.02632754,\n",
              "        -0.04480389,  0.05987517]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embed_weights.shape) #same size as vocab size and embedding dim (output dim of our embedding layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhwJFvlUBQPO",
        "outputId": "e6ddfa9e-286b-4a91-c623-0a900841e62d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have got the embedding matrix our model has learned to represent our tokens, lets see how we can visualize it. \n",
        "\n",
        "To do so, TensorFlow has a handy tool called projector :http://projector.tensorflow.org/\n",
        "\n",
        "And tensorflow also has a incredible guide on word embeddings themselves: https://www.tensorflow.org/text/guide/word_embeddings"
      ],
      "metadata": {
        "id": "Dlyd0GyfBnQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #create embedding files (we got this from tensorflwo word embeddings documentation)\n",
        "# import io\n",
        "# out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "# out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "# for index, word in enumerate(words_in_vocab):\n",
        "#   if index == 0:\n",
        "#     continue  # skip 0, it's padding.\n",
        "#   vec = embed_weights[index]\n",
        "#   out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "#   out_m.write(word + \"\\n\")\n",
        "# out_v.close()\n",
        "# out_m.close()"
      ],
      "metadata": {
        "id": "azplu1urCmI5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #download files from colab to projector (http://projector.tensorflow.org/)\n",
        "# try:\n",
        "#   from google.colab import files\n",
        "#   files.download('vectors.tsv')\n",
        "#   files.download('metadata.tsv')\n",
        "# except Exception:\n",
        "#   pass"
      ],
      "metadata": {
        "id": "gCqkdqS7Dl3Q"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the files above we can visualize them using http://projector.tensorflow.org/ and clicking the \"load\" button on the left hand same\n",
        "\n",
        ">Resouces: If you would like to know more about embeddings. I would encourage you to check out:\n",
        "* https://jalammar.github.io/illustrated-word2vec/\n",
        "* https://www.tensorflow.org/text/guide/word_embeddings"
      ],
      "metadata": {
        "id": "FAGOY19UEyTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks (RNN's)\n",
        "\n",
        "RNN are useful for sequence data.\n",
        "\n",
        "The premise of a recurrent neural network is to use the representation of a previous input to aid the representation of a later input.\n",
        "\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ],
      "metadata": {
        "id": "TvET9F_2Gkhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: LSTM\n",
        "\n",
        "LSTM= long short term memory (one of the most popular LSTM cells)\n",
        "\n",
        "Our structure of an RNN typically looks like this: \n",
        "\n",
        "```\n",
        "Input(text) -> Tokenize -> Embedding -> Layers (RNNs/dense) -> Output (label probabilities)\n",
        "```"
      ],
      "metadata": {
        "id": "lNmqISyGMFi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create an LSTM model\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "#print(x.shape)\n",
        "#x = layers.LSTM(64, return_sequences=True)(x) #when you are stacking RNN cells together, you need to set return_sequences\n",
        "#print(x.shape)\n",
        "x = layers.LSTM(64)(x)\n",
        "#print(x.shape)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
      ],
      "metadata": {
        "id": "Ishj2fcYOPeW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a summary\n",
        "model_2.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArK5VLyiPImr",
        "outputId": "1799a5c1-4c15-4476-bfef-4df15c0c5d31"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                49408     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,333,633\n",
            "Trainable params: 1,333,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_2.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "An1rBoWTRPqq"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "model_2_history = model_2.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
        "                                                                     \"model_2_LSTM\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxpspGCaRc9g",
        "outputId": "968b0eed-3057-4623-c402-3e859a248e8a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_2_LSTM/20220307-063456\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 7s 14ms/step - loss: 0.2224 - accuracy: 0.9207 - val_loss: 0.5346 - val_accuracy: 0.7835\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.1578 - accuracy: 0.9412 - val_loss: 0.6062 - val_accuracy: 0.7900\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.1279 - accuracy: 0.9534 - val_loss: 0.7002 - val_accuracy: 0.7756\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.1060 - accuracy: 0.9603 - val_loss: 0.6309 - val_accuracy: 0.7730\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.0853 - accuracy: 0.9653 - val_loss: 1.0542 - val_accuracy: 0.7822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make prediction with LSTM model\n",
        "model_2_pred_probs = model_2.predict(val_sentences)\n",
        "model_2_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngsYARGUSFk7",
        "outputId": "8b74bb5f-f5ef-4e2f-eade-a26158039b55"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.4061612e-03],\n",
              "       [7.1712583e-01],\n",
              "       [9.9996197e-01],\n",
              "       [3.4898542e-02],\n",
              "       [1.5701065e-04],\n",
              "       [9.9975759e-01],\n",
              "       [9.4901133e-01],\n",
              "       [9.9998426e-01],\n",
              "       [9.9996316e-01],\n",
              "       [6.0889775e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert model 2 pred probs to labels\n",
        "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
        "model_2_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka55bJA_SWtQ",
        "outputId": "f6c9eec8-c389-419e-af2a-49c8b0efc327"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate model 2 results\n",
        "model_2_results= calculate_results(y_true=val_labels,\n",
        "                                   y_pred= model_2_preds)"
      ],
      "metadata": {
        "id": "nkuB1cWISlm5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0XdjM5PS0Cg",
        "outputId": "9b445fca-5e6a-4c6c-d47d-a298276a882f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 78.21522309711287,\n",
              " 'f1': 0.7810542771172692,\n",
              " 'precision': 0.782423264149605,\n",
              " 'recall': 0.7821522309711286}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKeIYcVKTD6w",
        "outputId": "15e57ef4-6326-42d5-ba17-cdca3313515b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'f1': 0.7862189758049549,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: GRU\n",
        "\n",
        "Another popular and effective RNN component is the GRU or Gated Recurrent Unit.\n",
        "\n",
        "The GRU cell has similar features to an LSTM cell but has less parameters"
      ],
      "metadata": {
        "id": "QCNhFsTsTdud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build an RNN using the GRU cell\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.GRU(64)(x)\n",
        "#x = layers.GRU(64, return_sequences=True)(x) #put return_sequences=True if stacking the recurrent layers\n",
        "# x = layers.LSTM(64,return_sequences=True)(x)\n",
        "# x = layers.GRU(64)(x)\n",
        "# x = layers.Dense(64,activation=\"relu\")(x)\n",
        "\n",
        "outputs = layers.Dense(1,activation = \"sigmoid\")(x)\n",
        "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
      ],
      "metadata": {
        "id": "gxljM0DoT9aT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6iooVs0VQvR",
        "outputId": "af3b0bfe-8f78-415d-b7a5-95489fea30ec"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3_GRU\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 64)                37248     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,317,313\n",
            "Trainable params: 1,317,313\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_3.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "rtHE26azWPVA"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "model_3_history = model_3.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data= (val_sentences,val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
        "                                                                     \"model_3_GRU\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmbqmjY_XD1Q",
        "outputId": "f05db7a7-54fd-4c88-a33c-1ee4187a8858"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_3_GRU/20220307-063514\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 5s 13ms/step - loss: 0.1525 - accuracy: 0.9402 - val_loss: 0.7177 - val_accuracy: 0.7730\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.0839 - accuracy: 0.9691 - val_loss: 0.8730 - val_accuracy: 0.7703\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.0712 - accuracy: 0.9737 - val_loss: 0.9268 - val_accuracy: 0.7730\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.0620 - accuracy: 0.9752 - val_loss: 1.0487 - val_accuracy: 0.7730\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.0556 - accuracy: 0.9764 - val_loss: 1.2808 - val_accuracy: 0.7690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make prediction with our GRU model\n",
        "model_3_pred_probs = model_3.predict(val_sentences)\n",
        "model_3_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yLUcK3kXa2_",
        "outputId": "8017e721-0718-42c1-8518-93a5e6554dcb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.7927280e-04],\n",
              "       [8.6796516e-01],\n",
              "       [9.9990821e-01],\n",
              "       [1.9574376e-02],\n",
              "       [5.5804830e-05],\n",
              "       [9.9985278e-01],\n",
              "       [9.1045368e-01],\n",
              "       [9.9997711e-01],\n",
              "       [9.9994886e-01],\n",
              "       [9.8730451e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert model 3 pred probs to labels\n",
        "model_3_preds= tf.squeeze(tf.round(model_3_pred_probs))"
      ],
      "metadata": {
        "id": "C1kXDw4QXmCB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jip1hICWXx1S",
        "outputId": "bc276d44-ea5b-4835-e2e2-8c1d35c4f9ab"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate model 3 result\n",
        "model_3_results = calculate_results(y_true=val_labels,\n",
        "                                    y_pred =model_3_preds)\n",
        "\n",
        "model_3_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg9Cl3sBX387",
        "outputId": "54abbbc6-8a1e-43ee-cd4d-2f591c86fb82"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 76.9028871391076,\n",
              " 'f1': 0.7675415573053368,\n",
              " 'precision': 0.769557843731072,\n",
              " 'recall': 0.7690288713910761}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: Bidirectional RNN\n",
        "\n",
        "Normal RNN's go from left to right (just like you would read an english snetences), however bidirectional RNN goes from right to left as well as left to right.\n",
        "\n",
        "Refer: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional"
      ],
      "metadata": {
        "id": "ytHtCSUDYGVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a biirectional RNN in tensorflow\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "# x = layers.Bidirectional(layers.GRU(64))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_bidirectional\")"
      ],
      "metadata": {
        "id": "_Zh-sN_6_gYz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get summary \n",
        "model_4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_951BsNCCOK",
        "outputId": "8fa534b1-3903-4bc9-d4a0-5d7982a5d97a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4_bidirectional\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              98816     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,378,945\n",
            "Trainable params: 1,378,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_4.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "iFadBa0ZCN6Z"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "model_4_history=model_4.fit(train_sentences,\n",
        "                            train_labels,\n",
        "                            epochs=5,\n",
        "                            validation_data=(val_sentences, val_labels),\n",
        "                            callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
        "                                                                   \"model_4_bidirectional\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVhrnAQBDVwU",
        "outputId": "0e7cef35-dda4-41a2-e655-7ac047701ac7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/model_4_bidirectional/20220307-063538\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 9s 20ms/step - loss: 0.1092 - accuracy: 0.9692 - val_loss: 0.9686 - val_accuracy: 0.7625\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.0512 - accuracy: 0.9775 - val_loss: 1.2190 - val_accuracy: 0.7598\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.0478 - accuracy: 0.9772 - val_loss: 1.3059 - val_accuracy: 0.7598\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 4s 21ms/step - loss: 0.0462 - accuracy: 0.9803 - val_loss: 1.4398 - val_accuracy: 0.7625\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 5s 22ms/step - loss: 0.0407 - accuracy: 0.9810 - val_loss: 1.4386 - val_accuracy: 0.7664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make prediction\n",
        "model_4_pred_probs=model_4.predict(val_sentences)"
      ],
      "metadata": {
        "id": "pbhRMc0uDxzQ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmXfdb40D_SL",
        "outputId": "e1849b18-87b8-4f18-98c8-c755be6911b3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.7415341e-05],\n",
              "       [8.4615386e-01],\n",
              "       [9.9993932e-01],\n",
              "       [8.1425570e-02],\n",
              "       [8.8139086e-06],\n",
              "       [9.9779618e-01],\n",
              "       [8.6638820e-01],\n",
              "       [9.9996746e-01],\n",
              "       [9.9994516e-01],\n",
              "       [9.8958939e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert pred probs to pred labels\n",
        "model_4_preds= tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me_s_5TUECG6",
        "outputId": "8b79254f-b3e6-47a0-c0e3-e7e875371ff6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calcualte the results of our bidirectional model\n",
        "model_4_results = calculate_results(y_true=val_labels,\n",
        "                                    y_pred=model_4_preds)"
      ],
      "metadata": {
        "id": "oxDVv0A1EKCl"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcgqtpC7ETGl",
        "outputId": "36c4716a-cbd8-4150-f281-013c125f90aa"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 76.64041994750657,\n",
              " 'f1': 0.7641523881052159,\n",
              " 'precision': 0.7681630705252752,\n",
              " 'recall': 0.7664041994750657}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network for text (and other types of sequences)\n",
        "\n",
        "we have used CNNs for images are typically 2D (height x width).. however, our text data is 1D.\n",
        "\n",
        "Conv2D we ahve Conv2D for our image data but now we are gooing to use Conv1D.\n",
        "\n",
        "The typical structure of a Conv1D model for sequences (in our case, text):\n",
        "```\n",
        "Inputs (text)-> Tokenization -> Embedding -> Layer(s) -> (typically Conv1D +pooling) -> Outputs (class probabilities)\n",
        "```"
      ],
      "metadata": {
        "id": "Z6ysqaVrEpDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 5: Conv1D\n",
        "\n",
        "refer: https://poloclub.github.io/cnn-explainer/"
      ],
      "metadata": {
        "id": "QnDxuuhWG45f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out our embedding layer, Conv1D layer and max pooling\n",
        "from tensorflow.keras import layers\n",
        "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) #turn target sentence into embedding\n",
        "conv_1d= layers.Conv1D(filters=32,\n",
        "                       kernel_size=5, #this is also referred to as an ngram of 5\n",
        "                       strides=1, #default\n",
        "                       activation=\"relu\",\n",
        "                       padding=\"valid\") #default=\"valid\", the output is smaller than input\n",
        "\n",
        "conv_1d_output = conv_1d(embedding_test) #pas test embedding through conv1d layer\n",
        "max_pool = layers.GlobalMaxPool1D()\n",
        "max_pool_output=max_pool(conv_1d_output) #equivalent to \"get the most importnat feature\" or get the highest value\n",
        "\n",
        "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1eI-BdPG-W5",
        "outputId": "3d841700-f0f8-4851-f517-785d272c458e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create 1-dimensional convolutional layer to model sequences\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype= tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.Conv1D(filters=64, kernel_size=5, strides=1, activation=\"relu\", padding=\"valid\")(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "\n",
        "#x = layers.Dense(64, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
        "\n",
        "#compile the Conv1D\n",
        "model_5.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "#get a summary of our Conv1D model\n",
        "model_5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5gd0n4sHFfE",
        "outputId": "bb38c8fe-c98f-4655-feeb-4a451c412251"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5_Conv1D\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 11, 64)            41024     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Glo  (None, 64)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,321,089\n",
            "Trainable params: 1,321,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "model_5_history = model_5.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data = (val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
        "                                                                     \"Conv1D\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvZjBn9_HPDP",
        "outputId": "27bc4968-5a32-4ca2-aa42-97f6352661a6"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/Conv1D/20220307-064525\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 7s 17ms/step - loss: 0.1229 - accuracy: 0.9612 - val_loss: 0.9073 - val_accuracy: 0.7703\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.0732 - accuracy: 0.9729 - val_loss: 1.0721 - val_accuracy: 0.7677\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.0591 - accuracy: 0.9761 - val_loss: 1.1966 - val_accuracy: 0.7664\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 2s 8ms/step - loss: 0.0541 - accuracy: 0.9758 - val_loss: 1.2196 - val_accuracy: 0.7546\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.0513 - accuracy: 0.9790 - val_loss: 1.2392 - val_accuracy: 0.7572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make some predictions with our Conv1D model\n",
        "model_5_pred_probs = model_5.predict(val_sentences)\n",
        "model_5_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWvNrDykIJVH",
        "outputId": "7d516158-4b22-4bb7-fdf2-4a5069f31d12"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.9445341e-02],\n",
              "       [8.7502551e-01],\n",
              "       [9.9957389e-01],\n",
              "       [5.7515632e-02],\n",
              "       [6.3746853e-08],\n",
              "       [9.8988706e-01],\n",
              "       [9.2469066e-01],\n",
              "       [9.9997437e-01],\n",
              "       [9.9999928e-01],\n",
              "       [8.6355776e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert model_5 to labels\n",
        "model_5_preds= tf.squeeze(tf.round(model_5_pred_probs))\n",
        "model_5_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgKZi2aNITwC",
        "outputId": "5cdc6fdc-2aa7-4b0f-8805-ca996f35426a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model 5 predictions\n",
        "model_5_results = calculate_results(y_true = val_labels,\n",
        "                                    y_pred = model_5_preds)\n",
        "\n",
        "model_5_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRXhCL3AIcVg",
        "outputId": "109069ad-45ec-44db-c21d-ea4c6b952413"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 75.7217847769029,\n",
              " 'f1': 0.7550840315229969,\n",
              " 'precision': 0.758370695800474,\n",
              " 'recall': 0.7572178477690289}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP dataset resources: https://huggingface.co/models"
      ],
      "metadata": {
        "id": "-534_TZOIo5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 6: TensoorFlow Hub Pretrained Sentence Encoder\n",
        "\n",
        "Now we have built a few of our own models, let's try and use transfer learning for NLP, specifically using TensorFlwo Hub's universal sentence encoder: https://tfhub.dev/google/universal-sentence-encoder/4\n",
        "\n",
        "see how USE was created here: https://arxiv.org/abs/1803.11175v2"
      ],
      "metadata": {
        "id": "tVslmHP-LLbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "embed_samples = embed([sample_sentence,\n",
        "                       \"When you call the universal sentence encoder on a sentece, it turns it into numbers\"])\n",
        "print(embed_samples[0][:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsg_QpejMKT5",
        "outputId": "1e4a8111-4221-4637-a64b-6fa8bb717dfb"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[-0.01157024  0.0248591   0.0287805  -0.01271502  0.03971543  0.08827759\n",
            "  0.02680986  0.05589837 -0.01068731 -0.0059729   0.00639324 -0.01819523\n",
            "  0.00030817  0.09105891  0.05874644 -0.03180627  0.01512476 -0.05162928\n",
            "  0.00991369 -0.06865346 -0.04209306  0.0267898   0.03011008  0.00321069\n",
            " -0.00337969 -0.04787359  0.02266718 -0.00985924 -0.04063614 -0.01292095\n",
            " -0.04666384  0.056303   -0.03949255  0.00517685  0.02495828 -0.07014439\n",
            "  0.02871508  0.04947682 -0.00633971 -0.08960191  0.02807117 -0.00808362\n",
            " -0.01360601  0.05998649 -0.10361786 -0.05195372  0.00232955 -0.02332528\n",
            " -0.03758105  0.0332773 ], shape=(50,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_samples[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i47DbX2MvSw",
        "outputId": "2e559d23-49fa-4c03-a70e-a5f837befe9e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([512])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create keras layer using USE pretrained layer from tensorflow hub\n",
        "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        input_shape=[],\n",
        "                                        dtype = tf.string,\n",
        "                                        trainable=False,\n",
        "                                        name=\"USE\")"
      ],
      "metadata": {
        "id": "BXIdHHTvNIFd"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create model using the sequential API\n",
        "model_6 = tf.keras.Sequential([\n",
        "                               sentence_encoder_layer,\n",
        "                               layers.Dense(64, activation=\"relu\",),\n",
        "                               layers.Dense(1, activation=\"sigmoid\")\n",
        "], name=\"model_6_USE\")\n",
        "\n",
        "#compile\n",
        "model_6.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "model_6.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB4Hn59POhB5",
        "outputId": "8455bb10-aa76-4745-9e1e-82c49e8e845c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6_USE\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " USE (KerasLayer)            (None, 512)               256797824 \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                32832     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 256,830,721\n",
            "Trainable params: 32,897\n",
            "Non-trainable params: 256,797,824\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train a classifier on top of USE pretrained embeddings\n",
        "model_6_history = model_6.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
        "                                                                      \"tf_hub_sentences_encoder\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzrVF9NWPPH1",
        "outputId": "6867065a-10dd-4743-8782-c32ad1aeee5d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/tf_hub_sentences_encoder/20220307-072311\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 9s 33ms/step - loss: 0.5107 - accuracy: 0.7797 - val_loss: 0.4554 - val_accuracy: 0.7900\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 5s 23ms/step - loss: 0.4166 - accuracy: 0.8152 - val_loss: 0.4438 - val_accuracy: 0.8045\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 5s 22ms/step - loss: 0.4022 - accuracy: 0.8208 - val_loss: 0.4337 - val_accuracy: 0.8071\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 5s 23ms/step - loss: 0.3939 - accuracy: 0.8243 - val_loss: 0.4316 - val_accuracy: 0.8097\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 5s 22ms/step - loss: 0.3864 - accuracy: 0.8285 - val_loss: 0.4288 - val_accuracy: 0.8136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make predictions with USE TF hub model\n",
        "model_6_pred_probs = model_6.predict(val_sentences)\n",
        "model_6_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzpLSQT8PmoV",
        "outputId": "8a7777a2-da04-4655-eac8-722c73458158"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1556904 ],\n",
              "       [0.7969441 ],\n",
              "       [0.98698515],\n",
              "       [0.191221  ],\n",
              "       [0.72244304],\n",
              "       [0.7513382 ],\n",
              "       [0.98018485],\n",
              "       [0.9787436 ],\n",
              "       [0.93944544],\n",
              "       [0.07929484]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert predictions probs to labels\n",
        "model_6_pred = tf.squeeze(tf.round(model_6_pred_probs))\n",
        "model_6_pred[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUHBaazbP8Ex",
        "outputId": "0840cd52-ca5c-4ff5-c733-f3b52d7b1290"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate model 6 performance metrics\n",
        "model_6_results = calculate_results(y_true=val_labels,\n",
        "                                    y_pred=model_6_pred)\n",
        "\n",
        "model_6_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snoSDqu6QJpw",
        "outputId": "28ed0e08-a660-4e40-9220-0b10497ef33d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 81.36482939632546,\n",
              " 'f1': 0.8120618868299143,\n",
              " 'precision': 0.8160661319598898,\n",
              " 'recall': 0.8136482939632546}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhzVSBExQVyr",
        "outputId": "4145f2d6-c8ee-4b1f-da5e-efb6978030a9"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'f1': 0.7862189758049549,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706}"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jlQksEb_QiZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}